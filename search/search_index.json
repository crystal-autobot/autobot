{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Autobot","text":"<p>Ultra-efficient personal AI assistant powered by Crystal</p> <p>2MB binary \u00b7 ~5MB RAM \u00b7 &lt;20ms startup \u00b7 Zero runtime dependencies</p>"},{"location":"#why-autobot","title":"Why Autobot?","text":"<p>Inspired by OpenClaw \u2014 rebuilt in Crystal with security and efficiency first.</p> <p>2.0MB binary, ~5MB RAM, boots in under 20ms, zero runtime dependencies. Run dozens of bots on a single machine \u2014 each with its own personality, workspace, and config.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-Provider LLM \u2014 Anthropic, OpenAI, DeepSeek, Groq, Gemini, OpenRouter, AWS Bedrock, vLLM</li> <li>Chat Channels \u2014 Telegram, Slack, WhatsApp with allowlists and custom slash commands</li> <li>Vision \u2014 Send photos via Telegram and get AI-powered image analysis</li> <li>Voice \u2014 Voice messages auto-transcribed via Whisper (Groq/OpenAI)</li> <li>Kernel Sandbox \u2014 Docker/bubblewrap OS-level isolation, not regex path checks</li> <li>Memory \u2014 JSONL sessions with consolidation and persistent long-term memory</li> <li>Cron \u2014 Cron expressions, intervals, one-time triggers, per-owner isolation</li> <li>Extensible \u2014 Plugins, bash auto-discovery, markdown skills, subagents</li> <li>Observable \u2014 Token tracking, credential sanitization, audit trails</li> <li>Multi-Bot \u2014 Isolated directories per bot, run dozens on one machine</li> </ul>"},{"location":"#production-grade-security","title":"Production-Grade Security","text":"<p>Autobot uses kernel-enforced sandboxing via Docker or bubblewrap \u2014 not application-level validation. When the LLM executes commands:</p> <ul> <li>Only workspace directory is accessible (enforced by Linux mount namespaces)</li> <li>Everything else is invisible to the LLM \u2014 your <code>/home</code>, <code>/etc</code>, system files simply don't exist</li> <li>No symlink exploits, TOCTOU, or path traversal \u2014 kernel guarantees workspace isolation</li> <li>Process isolation \u2014 LLM can't see or interact with host processes</li> </ul> <p>Learn more about Security</p> <p>Architecture Overview</p>"},{"location":"#providers","title":"Providers","text":"<p>Autobot works with all major LLM providers \u2014 pick the one that fits your needs:</p> Provider Best for Anthropic Claude models via native API OpenAI GPT-5 family, Whisper transcription DeepSeek Low-cost, strong reasoning Groq Ultra-fast inference, free tier Google Gemini Gemini Pro and Flash OpenRouter Hundreds of models, one API key AWS Bedrock Enterprise, IAM-based access vLLM / Local Full privacy, self-hosted <p>All providers</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># macOS (Homebrew)\nbrew tap crystal-autobot/tap\nbrew install autobot\n\n# Create and run a bot\nautobot new optimus\ncd optimus\nautobot agent\n</code></pre> <p>Full Quick Start Guide</p>"},{"location":"anthropic/","title":"Anthropic","text":"<p>Autobot supports Anthropic as an LLM provider via the Messages API. This gives access to the Claude model family including Claude Opus, Sonnet, and Haiku.</p>"},{"location":"anthropic/#setup","title":"Setup","text":""},{"location":"anthropic/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at console.anthropic.com.</p>"},{"location":"anthropic/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"Anthropic (Claude)\" as provider\n</code></pre>"},{"location":"anthropic/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"anthropic/claude-haiku-4-5\"\n\nproviders:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n</code></pre>"},{"location":"anthropic/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (anthropic)\n</code></pre>"},{"location":"anthropic/#model-naming","title":"Model naming","text":"<p>Models use the <code>anthropic/</code> prefix followed by the Anthropic model ID:</p> <pre><code># Latest (recommended)\nmodel: \"anthropic/claude-haiku-4-5\"     # Fast, cheap, capable (recommended default)\nmodel: \"anthropic/claude-sonnet-4-6\"    # Mid-tier \u2014 strong coding and reasoning\nmodel: \"anthropic/claude-opus-4-6\"      # Flagship \u2014 smartest, 1M context\n\n# Previous generation (still available)\nmodel: \"anthropic/claude-sonnet-4-5\"\nmodel: \"anthropic/claude-opus-4-5\"\nmodel: \"anthropic/claude-sonnet-4\"\nmodel: \"anthropic/claude-opus-4\"\n</code></pre> <p><code>claude-haiku-4-5</code> is a good default for most use cases \u2014 fast, affordable ($1/$5 per 1M tokens), and capable enough for tool use, coding, and everyday tasks. Use Sonnet or Opus when you need stronger reasoning or larger context.</p> <p>The <code>anthropic/</code> prefix tells autobot to route to the Anthropic Messages API. It is stripped before sending to the API.</p> <p>See the full model list in the Anthropic docs.</p>"},{"location":"anthropic/#custom-api-base","title":"Custom API base","text":"<p>To use a self-hosted proxy or alternative endpoint:</p> <pre><code>providers:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n    api_base: \"https://your-proxy.example.com/v1/messages\"\n</code></pre>"},{"location":"anthropic/#extra-headers","title":"Extra headers","text":"<p>Pass additional headers with every request (e.g. for proxy authentication):</p> <pre><code>providers:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n    extra_headers:\n      X-Custom-Header: \"value\"\n</code></pre>"},{"location":"anthropic/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 Anthropic API key (<code>sk-ant-...</code>) <code>api_base</code> No <code>https://api.anthropic.com/v1/messages</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"anthropic/#how-it-works","title":"How it works","text":"<p>Anthropic uses its own Messages API format, which differs from the OpenAI-compatible standard:</p> <ul> <li><code>x-api-key</code> header for authentication (not <code>Authorization: Bearer</code>)</li> <li><code>anthropic-version</code> header sent with every request (<code>2023-06-01</code>)</li> <li>System prompt extracted from messages and sent as a top-level <code>system</code> field</li> <li>Tool use blocks use <code>tool_use</code> / <code>tool_result</code> format instead of <code>function</code></li> </ul> <p>Autobot detects Anthropic models automatically and handles all format conversion transparently. Tools, MCP servers, plugins, and all other features work the same as with other providers.</p>"},{"location":"anthropic/#voice-transcription","title":"Voice transcription","text":"<p>Anthropic does not provide a transcription API. If you need voice message support, configure an additional Groq or OpenAI provider for Whisper-based transcription.</p>"},{"location":"anthropic/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Text and image content only \u2014 Document and audio content blocks are not supported.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> </ul>"},{"location":"anthropic/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://api.anthropic.com/v1/messages model=... (anthropic)</code> \u2014 confirms native Anthropic path</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"anthropic/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: authentication_error\" \u2014 Invalid or expired API key. Verify at console.anthropic.com.</p> <p>\"API error: rate_limit_error\" \u2014 Too many requests. Anthropic applies per-key rate limits \u2014 check your plan's limits.</p> <p>\"API error: overloaded_error\" \u2014 The API is temporarily overloaded. Retry after a moment.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Autobot is organized around a message-driven agent loop. Channels receive user input, the agent loop orchestrates LLM interactions and tool execution, and responses flow back through the same channel.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>graph LR\n    subgraph Channels[\"Channels\"]\n        direction TB\n        CLI[CLI]\n        TG[Telegram]\n        SL[Slack]\n        WA[WhatsApp]\n        ZP[Zulip]\n    end\n\n    BUS((Event Bus))\n\n    subgraph Agent[\"Agent Core\"]\n        direction TB\n        LOOP[Agent Loop]\n        CTX[Context Builder]\n        MEM[Memory]\n        SESS[Sessions]\n    end\n\n    LLM[LLM Provider\\nAnthropic \u00b7 OpenAI\\nDeepSeek \u00b7 Groq \u00b7 Gemini]\n\n    subgraph Tools[\"Tools\"]\n        direction TB\n        BASH[Bash]\n        FS[Filesystem]\n        WEB[Web Search]\n        MCP[MCP Servers]\n        SPAWN[Subagents]\n        PLG[Plugins]\n    end\n\n    SAND[Sandbox\\nDocker \u00b7 Bubblewrap]\n    CRON[Cron\\nScheduler]\n\n    CLI &amp; TG &amp; SL &amp; WA &amp; ZP --&gt; BUS\n    CRON -.-&gt;|scheduled| BUS\n    BUS --&gt; LOOP\n    LOOP --- CTX &amp; MEM &amp; SESS\n    LOOP --&gt;|request| LLM\n    LLM --&gt;|tool calls| BASH &amp; FS &amp; WEB &amp; MCP &amp; SPAWN &amp; PLG\n    BASH &amp; FS --&gt; SAND\n    LOOP -.-&gt;|response| BUS\n\n    style BUS fill:#7c4dff,stroke:#651fff,color:#fff\n    style LOOP fill:#5c6bc0,stroke:#3949ab,color:#fff\n    style CTX fill:#7986cb,stroke:#5c6bc0,color:#fff\n    style MEM fill:#7986cb,stroke:#5c6bc0,color:#fff\n    style SESS fill:#7986cb,stroke:#5c6bc0,color:#fff\n    style LLM fill:#26a69a,stroke:#00897b,color:#fff\n    style CLI fill:#42a5f5,stroke:#1e88e5,color:#fff\n    style TG fill:#42a5f5,stroke:#1e88e5,color:#fff\n    style SL fill:#42a5f5,stroke:#1e88e5,color:#fff\n    style WA fill:#42a5f5,stroke:#1e88e5,color:#fff\n    style ZP fill:#42a5f5,stroke:#1e88e5,color:#fff\n    style BASH fill:#ffa726,stroke:#fb8c00,color:#fff\n    style FS fill:#ffa726,stroke:#fb8c00,color:#fff\n    style WEB fill:#ffa726,stroke:#fb8c00,color:#fff\n    style MCP fill:#ffa726,stroke:#fb8c00,color:#fff\n    style SPAWN fill:#ffa726,stroke:#fb8c00,color:#fff\n    style PLG fill:#ffa726,stroke:#fb8c00,color:#fff\n    style SAND fill:#ef5350,stroke:#e53935,color:#fff\n    style CRON fill:#ab47bc,stroke:#8e24aa,color:#fff</code></pre>"},{"location":"architecture/#request-lifecycle","title":"Request Lifecycle","text":"<ol> <li>Message ingress \u2014 A channel adapter (CLI, Telegram, Slack, WhatsApp, Zulip) receives user input and publishes it to the event bus.</li> <li>Context assembly \u2014 The agent loop picks up the message and builds the full LLM context: system prompt, conversation history from the session store, relevant memories, and available skills.</li> <li>LLM request \u2014 The assembled context is sent to the configured LLM provider (Anthropic, OpenAI, DeepSeek, Groq, Gemini, OpenRouter, or vLLM).</li> <li>Tool execution \u2014 If the LLM response contains tool calls, each tool is executed through the tool registry. Shell commands run inside a kernel-enforced sandbox (Docker or bubblewrap). MCP tools are proxied to external servers.</li> <li>Iteration \u2014 Tool results are fed back into the agent loop. The LLM can issue further tool calls, creating a multi-turn execution cycle until it produces a final text response.</li> <li>Message egress \u2014 The final response is published to the event bus and delivered back through the originating channel.</li> </ol>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#channels-channels","title":"Channels (<code>channels/</code>)","text":"<p>Channel adapters handle protocol-specific communication (HTTP webhooks, WebSocket, polling) and normalize messages into a common format. Each channel supports media downloads for vision and voice processing.</p> Channel Transport Features CLI stdin/stdout Interactive and single-command modes Telegram HTTP polling Photos, voice, custom commands, allowlists Slack WebSocket Thread support, file uploads WhatsApp HTTP webhooks Media messages Zulip HTTP Polling Private direct messages"},{"location":"architecture/#agent-loop-agent","title":"Agent Loop (<code>agent/</code>)","text":"<p>The core orchestration engine. When a message arrives, it builds context from bootstrap files (AGENTS.md, SOUL.md, USER.md), conversation history, and memory. It then enters a loop: call the LLM, execute any requested tools inside the sandbox, feed results back, and repeat \u2014 up to a configurable maximum of iterations. Once the LLM produces a final text response, the result is saved to the session and sent back to the channel.</p> <pre><code>graph LR\n    CHAT[\"\ud83d\udce1 Channels\\nTelegram \u00b7 Slack \u00b7 WhatsApp \u00b7 Zulip \u00b7 CLI\"]\n\n    CHAT --&gt; MSG[\"\ud83d\udcac Message\"]\n\n    subgraph Loop[\"Agent Loop\"]\n        MSG --&gt; LLM[\"\ud83e\udd16 LLM\"]\n        LLM --&gt; TOOLS[\"\ud83d\udd27 Tools\"]\n        TOOLS --&gt;|results| LLM\n        LLM --&gt;|done| RSP[\"\ud83d\udcac Response\"]\n    end\n\n    RSP --&gt; CHAT\n\n    subgraph Context\n        direction TB\n        MEM[\"\ud83e\udde0 Memory\"]\n        SKL[\"\u26a1 Skills\"]\n    end\n\n    Context &lt;--&gt; TOOLS\n\n    style MSG fill:#fff3cd,stroke:#ffc107,color:#333\n    style LLM fill:#ffe0b2,stroke:#ff9800,color:#333\n    style TOOLS fill:#ffe0b2,stroke:#ff9800,color:#333\n    style RSP fill:#ffcdd2,stroke:#ef5350,color:#333\n    style CHAT fill:#e3f2fd,stroke:#90caf9,color:#333\n    style MEM fill:#e8d5f5,stroke:#ab47bc,color:#333\n    style SKL fill:#e8d5f5,stroke:#ab47bc,color:#333</code></pre> <p>On each turn, the agent loop:</p> <ul> <li>Assembles context via the context builder (system prompt + history + memory + skills)</li> <li>Handles multimodal input (images, voice transcription)</li> <li>Manages the tool call loop (execute \u2192 feed results \u2192 repeat)</li> <li>Supports subagents for parallel or delegated tasks</li> <li>Triggers memory hooks for session consolidation</li> </ul>"},{"location":"architecture/#llm-providers-providers","title":"LLM Providers (<code>providers/</code>)","text":"<p>A unified HTTP interface to multiple LLM backends. All providers implement the same request/response contract, making model switching a config change. Token usage is tracked per request for observability.</p>"},{"location":"architecture/#tool-system-tools","title":"Tool System (<code>tools/</code>)","text":"<p>Tools are the agent's hands. The registry discovers and exposes tools to the LLM. Built-in tools include:</p> <ul> <li>Bash \u2014 Shell commands in a sandboxed environment</li> <li>Filesystem \u2014 Read/write files within the workspace</li> <li>Web \u2014 HTTP requests and web search</li> <li>Cron \u2014 Schedule recurring or one-time tasks</li> <li>Spawn \u2014 Launch subagents for parallel work</li> <li>Message \u2014 Send messages to channel owners</li> </ul> <p>All shell execution goes through the sandbox executor, which enforces OS-level isolation via Docker or bubblewrap.</p>"},{"location":"architecture/#event-bus-bus","title":"Event Bus (<code>bus/</code>)","text":"<p>Internal pub/sub system that decouples channels from the agent loop. Messages, responses, and scheduled events all flow through the bus, enabling multi-channel operation from a single agent instance.</p>"},{"location":"architecture/#session-store-session","title":"Session Store (<code>session/</code>)","text":"<p>JSONL-based conversation persistence. Each session captures the full message history, enabling context continuity across restarts. Sessions are scoped per owner for multi-user isolation.</p>"},{"location":"architecture/#memory-memory","title":"Memory (<code>memory/</code>)","text":"<p>Long-term memory with two tiers:</p> <ul> <li>Session memory \u2014 Automatic consolidation of conversation history</li> <li>Persistent memory \u2014 Facts and preferences that survive across sessions</li> </ul> <p>See Memory for details.</p>"},{"location":"architecture/#mcp-servers-mcp","title":"MCP Servers (<code>mcp/</code>)","text":"<p>Model Context Protocol client that connects external tool servers. Tools from MCP servers are auto-discovered and exposed to the LLM as <code>mcp_{server}_{tool}</code>.</p>"},{"location":"architecture/#plugins-plugins","title":"Plugins (<code>plugins/</code>)","text":"<p>Plugin system with bash auto-discovery and markdown skills. Plugins extend the agent's capabilities without modifying core code.</p>"},{"location":"architecture/#cron-cron","title":"Cron (<code>cron/</code>)","text":"<p>Scheduler supporting cron expressions, fixed intervals, and one-time triggers. When a job fires, it publishes a message through the event bus, triggering a full agent turn with all tools. See Cron &amp; Scheduling for details.</p>"},{"location":"bedrock/","title":"AWS Bedrock","text":"<p>Autobot supports AWS Bedrock as an LLM provider via the Converse API. This gives access to Claude, Amazon Nova, and other foundation models hosted on AWS with SigV4 authentication.</p>"},{"location":"bedrock/#setup","title":"Setup","text":""},{"location":"bedrock/#1-configure-credentials","title":"1. Configure credentials","text":"<p>Add AWS credentials to your <code>.env</code> file:</p> <pre><code>AWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=wJalr...\nAWS_REGION=us-east-1\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"AWS Bedrock\" as provider\n</code></pre>"},{"location":"bedrock/#2-configure-the-provider","title":"2. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n\nproviders:\n  bedrock:\n    access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    region: \"${AWS_REGION}\"\n</code></pre>"},{"location":"bedrock/#3-verify","title":"3. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (bedrock)\n</code></pre>"},{"location":"bedrock/#model-naming","title":"Model naming","text":"<p>Models must use the <code>bedrock/</code> prefix. After the prefix, use the Bedrock model ID:</p> <pre><code># Foundation models\nmodel: \"bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0\"\nmodel: \"bedrock/amazon.nova-pro-v1:0\"\n\n# Cross-region inference profiles\nmodel: \"bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n</code></pre> <p>The <code>bedrock/</code> prefix tells autobot to route to the Bedrock provider instead of the HTTP provider. It is stripped before sending to the API.</p>"},{"location":"bedrock/#guardrails","title":"Guardrails","text":"<p>Bedrock Guardrails can filter harmful content. Both <code>guardrail_id</code> and <code>guardrail_version</code> are required \u2014 setting only <code>guardrail_id</code> logs a warning and disables guardrails.</p> <pre><code>providers:\n  bedrock:\n    access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    region: \"${AWS_REGION}\"\n    guardrail_id: \"abc123def456\"\n    guardrail_version: \"1\"\n</code></pre> <p>When a guardrail intervenes, the response includes <code>finish_reason: \"guardrail_intervened\"</code> and the guardrail trace is logged at INFO level.</p>"},{"location":"bedrock/#session-tokens","title":"Session tokens","text":"<p>For temporary credentials (e.g. from <code>aws sts assume-role</code>), add <code>session_token</code>:</p> <pre><code>providers:\n  bedrock:\n    access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    session_token: \"${AWS_SESSION_TOKEN}\"\n    region: \"${AWS_REGION}\"\n</code></pre>"},{"location":"bedrock/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>access_key_id</code> Yes \u2014 AWS access key ID <code>secret_access_key</code> Yes \u2014 AWS secret access key <code>region</code> No <code>us-east-1</code> AWS region <code>session_token</code> No \u2014 Temporary session token (STS) <code>guardrail_id</code> No \u2014 Bedrock guardrail identifier <code>guardrail_version</code> No \u2014 Guardrail version (required if guardrail_id is set)"},{"location":"bedrock/#how-it-works","title":"How it works","text":"<p>Unlike other providers that use OpenAI-compatible HTTP APIs, Bedrock uses:</p> <ul> <li>AWS SigV4 authentication (no API keys)</li> <li>Converse API endpoint (<code>/model/{modelId}/converse</code>)</li> <li>Union-key content blocks (<code>{\"text\": \"...\"}</code> instead of <code>{\"type\": \"text\", ...}</code>)</li> <li>camelCase field names (<code>toolUse</code>, <code>toolResult</code>, <code>inputSchema</code>)</li> </ul> <p>Autobot handles all format conversion transparently. Tools, MCP servers, plugins, and all other features work the same as with other providers.</p>"},{"location":"bedrock/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating. The <code>ConverseStream</code> API is not used.</li> <li>No credential refresh \u2014 SigV4 credentials are set at startup. Temporary credentials (STS session tokens) will not refresh when they expire; restart autobot to pick up new credentials.</li> <li>Text-only content blocks \u2014 Image and document content blocks in messages and tool results are not converted. Only text is sent to Bedrock.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> </ul>"},{"location":"bedrock/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>Bedrock: region=... model=...</code> \u2014 confirms provider is active</li> <li><code>Bedrock toolConfig: N tools</code> \u2014 confirms tools are in the request</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"bedrock/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Model must start with <code>bedrock/</code> prefix.</p> <p>\"HTTP 403: ...\" \u2014 Check IAM permissions. The role needs <code>bedrock:InvokeModel</code> on the model ARN.</p> <p>\"HTTP 404: ...\" \u2014 Model not available in the configured region. Try a cross-region inference profile (e.g. <code>us.anthropic.claude-...</code>).</p> <p>guardrail_id without guardrail_version \u2014 Both are required. Check logs for the warning message.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Default config path: <code>./config.yml</code></p> <p>Config precedence:</p> <ol> <li><code>--config &lt;path&gt;</code></li> <li><code>./config.yml</code></li> <li>Schema defaults</li> </ol>"},{"location":"configuration/#minimal-config","title":"Minimal Config","text":"<pre><code>providers:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n\nagents:\n  defaults:\n    model: \"anthropic/claude-sonnet-4-5\"\n</code></pre>"},{"location":"configuration/#providers","title":"Providers","text":"<p>Supported provider blocks \u2014 see the Providers overview for a full comparison.</p> <ul> <li><code>anthropic</code> \u2014 Claude models via Messages API</li> <li><code>openai</code> \u2014 GPT-5 family, o3 models</li> <li><code>deepseek</code> \u2014 DeepSeek-V3, R1</li> <li><code>groq</code> \u2014 Ultra-fast inference (Llama, Mixtral, Gemma)</li> <li><code>gemini</code> \u2014 Google Gemini Pro, Flash</li> <li><code>openrouter</code> \u2014 Gateway to hundreds of models</li> <li><code>bedrock</code> \u2014 AWS Bedrock via Converse API</li> <li><code>vllm</code> \u2014 Local/hosted OpenAI-compatible endpoint</li> </ul>"},{"location":"configuration/#voice-transcription","title":"Voice Transcription","text":"<p>Voice messages are automatically transcribed using the Whisper API when a supported provider is configured. No extra settings needed \u2014 the API key is reused from the provider config.</p> <ul> <li>Groq (preferred \u2014 faster, free tier): uses <code>whisper-large-v3-turbo</code></li> <li>OpenAI: uses <code>whisper-1</code></li> </ul> <p>If neither Groq nor OpenAI is configured, voice messages fall back to <code>[voice message]</code> text.</p>"},{"location":"configuration/#channels","title":"Channels","text":"<p>Security Note: <code>allow_from</code> is deny-by-default for security.</p> <p>Setup guides: Telegram | Slack | Zulip</p> <pre><code>channels:\n  telegram:\n    enabled: false\n    token: \"\"\n    # allow_from options:\n    # []              - DENY ALL (secure default)\n    # [\"*\"]           - Allow anyone (use with caution)\n    # [\"@user\", \"id\"] - Allowlist specific users (recommended)\n    allow_from: []\n\n  slack:\n    enabled: false\n    bot_token: \"\"\n    app_token: \"\"\n    allow_from: []\n    mode: \"socket\"\n    group_policy: \"mention\"  # \"mention\" (secure) | \"open\" | \"allowlist\"\n\n  whatsapp:\n    enabled: false\n    bridge_url: \"ws://localhost:3001\"  # Prefer wss:// for production\n    # allow_from: []      - DENY ALL\n    # allow_from: [\"*\"]   - Allow anyone\n    # allow_from: [\"num\"] - Allowlist phone numbers\n    allow_from: []\n\n  zulip:\n    enabled: false\n    site: \"https://zulip.example.com\"\n    email: \"bot@zulip.example.com\"\n    api_key: \"\"\n    allow_from: []\n</code></pre>"},{"location":"configuration/#tools","title":"Tools","text":"<pre><code>tools:\n  sandbox: auto  # auto | bubblewrap | docker | none (default: auto)\n  docker_image: \"python:3.14-alpine\"  # optional, default: alpine:latest\n  exec:\n    timeout: 60\n  web:\n    search:\n      api_key: \"\"\n      max_results: 5\n  image:\n    enabled: true              # default: true\n    # provider: openai         # optional override (openai or gemini)\n    # model: gpt-image-1       # optional, auto-detected from provider\n    # size: 1024x1024          # default: 1024x1024\n</code></pre> <p>When sandboxed, all shell commands run inside the sandbox (bubblewrap or Docker). The kernel enforces workspace restrictions \u2014 pipes, redirects, and other shell features are safe to use because the process cannot access files outside the workspace regardless.</p>"},{"location":"configuration/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>Connect to external MCP servers to give the LLM access to remote tools (Garmin, GitHub, etc.).</p> <pre><code>mcp:\n  servers:\n    garmin:\n      command: \"uvx\"\n      args: [\"--python\", \"3.12\", \"--from\", \"git+https://github.com/Taxuspt/garmin_mcp\", \"garmin-mcp\"]\n      env:\n        GARMIN_EMAIL: \"${GARMIN_EMAIL}\"\n    github:\n      command: \"npx\"\n      args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n      env:\n        GITHUB_TOKEN: \"${GITHUB_TOKEN}\"\n</code></pre> <p>Tools are auto-discovered at startup and registered as <code>mcp_{server}_{tool}</code>. MCP servers run unsandboxed (they need network access) but with isolated env vars.</p> <p>-&gt; MCP Documentation</p>"},{"location":"configuration/#cron","title":"Cron","text":"<pre><code>cron:\n  enabled: true\n  store_path: \"./cron.json\"\n</code></pre>"},{"location":"configuration/#gateway","title":"Gateway","text":"<pre><code>gateway:\n  host: \"127.0.0.1\"  # Default: localhost only (change to 0.0.0.0 for external access)\n  port: 18790\n</code></pre>"},{"location":"configuration/#full-config-reference","title":"Full config reference","text":"<pre><code># LLM providers (configure at least one)\nproviders:\n  anthropic:\n    api_key: \"sk-ant-...\"\n  openai:\n    api_key: \"sk-...\"\n  deepseek:\n    api_key: \"...\"\n  groq:\n    api_key: \"...\"\n  gemini:\n    api_key: \"...\"\n  openrouter:\n    api_key: \"...\"\n  vllm:\n    api_base: \"http://localhost:8000\"\n    api_key: \"token\"\n  bedrock:\n    access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    region: \"${AWS_REGION}\"\n    # guardrail_id: \"abc123\"\n    # guardrail_version: \"1\"\n\n# Agent defaults\nagents:\n  defaults:\n    model: \"anthropic/claude-sonnet-4-5\"\n    max_tokens: 8192\n    temperature: 0.7\n    max_tool_iterations: 20\n    memory_window: 50\n    workspace: \"./workspace\"\n\n# Chat channels\nchannels:\n  telegram:\n    enabled: true\n    token: \"BOT_TOKEN\"\n    allow_from: [\"username1\", \"username2\"]\n    custom_commands:\n      macros:\n        # Simple format (command name used as description)\n        summarize: \"Summarize the last conversation in 3 bullet points\"\n        # Rich format (with custom description shown in Telegram command menu)\n        translate:\n          prompt: \"Translate the following to English\"\n          description: \"Translate text to English\"\n      scripts:\n        deploy:\n          path: \"/home/user/scripts/deploy.sh\"\n          description: \"Deploy to production\"\n        status: \"/home/user/scripts/check_status.sh\"\n\n  slack:\n    enabled: false\n    bot_token: \"xoxb-...\"\n    app_token: \"xapp-...\"\n    allow_from: [\"U12345678\"]\n    mode: \"socket\"\n    group_policy: \"mention\"\n    dm:\n      enabled: true\n      policy: \"open\"\n\n  whatsapp:\n    enabled: false\n    bridge_url: \"ws://localhost:3001\"\n    allow_from: [\"1234567890\"]\n\n  zulip:\n    enabled: false\n    site: \"https://zulip.example.com\"\n    email: \"bot@zulip.example.com\"\n    api_key: \"\"\n    allow_from: [\"you@example.com\"]\n\n# Tool settings\ntools:\n  sandbox: auto  # auto | bubblewrap | docker | none\n  docker_image: \"python:3.14-alpine\"  # optional, default: alpine:latest\n  web:\n    search:\n      api_key: \"BRAVE_API_KEY\"\n      max_results: 5\n  exec:\n    timeout: 60\n  image:\n    enabled: true\n    # provider: openai       # optional override (openai or gemini)\n    # model: gpt-image-1     # optional, auto-detected from provider\n    # size: 1024x1024\n  restrict_to_workspace: true  # Default: true (recommended for security)\n\n# Cron scheduler\ncron:\n  enabled: true\n  store_path: \"./cron.json\"\n\n# MCP servers (external tool providers)\nmcp:\n  servers:\n    garmin:\n      command: \"uvx\"\n      args: [\"--python\", \"3.12\", \"--from\", \"git+https://github.com/Taxuspt/garmin_mcp\", \"garmin-mcp\"]\n      env:\n        GARMIN_EMAIL: \"${GARMIN_EMAIL}\"\n    github:\n      command: \"npx\"\n      args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n      env:\n        GITHUB_TOKEN: \"${GITHUB_TOKEN}\"\n\n# Gateway API server\ngateway:\n  host: \"127.0.0.1\"  # Localhost only by default for security\n  port: 18790\n</code></pre>"},{"location":"cron/","title":"Cron &amp; Scheduled Tasks","text":"<p>Autobot includes a built-in scheduler for recurring checks, one-time reminders, and deferred tasks. Jobs fire as full agent turns with access to all tools.</p>"},{"location":"cron/#overview","title":"Overview","text":"<p>The cron system solves three common needs:</p> <ul> <li>Reminders \u2014 \"Remind me to drink water in 30 minutes\" (one-time deferred task)</li> <li>Recurring reports \u2014 \"Send me a weather summary every morning at 9am\" (cron expression)</li> <li>Periodic tasks \u2014 \"Check my email every 5 minutes\" (fixed interval)</li> </ul> <p>When a job fires, it triggers a full agent turn \u2014 the agent can use MCP tools, web search, memory, and any other registered tools to complete the task. The response is automatically delivered to the user.</p>"},{"location":"cron/#schedule-types","title":"Schedule Types","text":""},{"location":"cron/#fixed-interval-every_seconds","title":"Fixed Interval (<code>every_seconds</code>)","text":"<p>Runs repeatedly at a fixed interval.</p> <pre><code>\"Check my email every 5 minutes\"\n\u2192 every_seconds: 300\n</code></pre>"},{"location":"cron/#cron-expression-cron_expr","title":"Cron Expression (<code>cron_expr</code>)","text":"<p>Standard 5-field cron syntax: <code>MIN(0-59) HOUR(0-23) DOM(1-31) MON(1-12) DOW(0-6)</code></p> <p>Supports: <code>*</code> (any), ranges (<code>9-17</code>), steps (<code>*/5</code>), lists (<code>1,15,30</code>), combos (<code>1-30/10</code>), named months (<code>jan</code>-<code>dec</code>), named days (<code>mon</code>-<code>sun</code>), and shortcuts (<code>@hourly</code>, <code>@daily</code>, <code>@weekly</code>, <code>@monthly</code>, <code>@yearly</code>).</p> <p>All values must be integers. Minimum granularity is 1 minute \u2014 for sub-minute intervals, use <code>every_seconds</code>.</p> <pre><code>\"Send me a morning briefing at 9am\"\n\u2192 cron_expr: \"0 9 * * *\"\n\n\"Every 5 minutes during work hours on weekdays\"\n\u2192 cron_expr: \"*/5 9-17 * * 1-5\"\n</code></pre>"},{"location":"cron/#one-time-at","title":"One-Time (<code>at</code>)","text":"<p>Runs once at a specific time, then auto-deletes.</p> <pre><code>\"Remind me at 3pm to call the dentist\"\n\u2192 at: \"2026-02-20T15:00:00Z\"\n</code></pre>"},{"location":"cron/#how-it-works","title":"How It Works","text":""},{"location":"cron/#job-execution-flow","title":"Job Execution Flow","text":"<pre><code>graph LR\n    TIMER[Timer fires] --&gt; CHECK{Job due?}\n    CHECK --&gt;|yes| CALLBACK[on_job callback]\n    CALLBACK --&gt; BUS[Event Bus]\n    BUS --&gt; LOOP[Agent Loop]\n    LOOP --&gt; LLM[LLM + Tools]\n    LLM --&gt; USER[Response auto-delivered]\n\n    style TIMER fill:#ab47bc,stroke:#8e24aa,color:#fff\n    style BUS fill:#7c4dff,stroke:#651fff,color:#fff\n    style LOOP fill:#5c6bc0,stroke:#3949ab,color:#fff\n    style LLM fill:#26a69a,stroke:#00897b,color:#fff\n    style USER fill:#ffa726,stroke:#fb8c00,color:#fff</code></pre> <ol> <li>Timer fires \u2014 The cron service detects a job is due</li> <li>Publish to bus \u2014 An <code>InboundMessage</code> is published to the event bus with <code>channel: \"system\"</code> and <code>sender_id: \"cron:{job_id}\"</code></li> <li>Agent turn \u2014 The agent loop picks up the message and executes the job's prompt</li> <li>Tool execution \u2014 The agent uses any tools needed (MCP, web search, etc.) to fulfill the task</li> <li>Explicit delivery \u2014 The agent uses the <code>message</code> tool to send results to the user (no auto-delivery)</li> </ol>"},{"location":"cron/#background-turn-restrictions","title":"Background Turn Restrictions","text":"<p>Cron turns use a minimal system prompt and exclude certain tools to prevent unintended behavior:</p> <ul> <li><code>spawn</code> \u2014 excluded to prevent background task proliferation</li> </ul> <p>Cron turns never auto-deliver the final response. The agent must use the <code>message</code> tool explicitly to notify the user. This enables conditional delivery \u2014 the agent can stay silent when there is nothing to report (e.g., \"monitor X, only notify if Y changes\").</p> <p>The <code>cron</code> tool is available so jobs can self-remove when their task defines a stop condition (e.g., \"monitor X until condition Y, then stop\").</p>"},{"location":"cron/#exec-payloads","title":"Exec Payloads","text":"<p>Many monitoring and automation tasks are deterministic \u2014 check a URL, compare a value, send an alert. These don't need an LLM reasoning about what to do every time. But setting them up still requires figuring out the right shell command, cron expression, and wiring.</p> <p>Exec payloads let you describe what you want in natural language \u2014 the agent writes the shell command once, and from that point on it runs directly on a schedule with zero LLM involvement. No tokens, no latency, no cost per execution.</p> <p>User: \"Check air quality in NYC every hour, notify me if AQI goes above 100\"</p> <p>The agent creates an exec job with the right <code>curl | jq</code> pipeline. Every hour the command runs on its own \u2014 the LLM is never called again.</p>"},{"location":"cron/#how-exec-differs-from-agent-turns","title":"How exec differs from agent turns","text":"Agent turn Exec payload Setup LLM writes the prompt LLM writes the shell command Each run Full LLM turn with tools Shell command only Token cost Per-invocation Zero (after setup) Output LLM-generated response Raw stdout Best for Tasks needing reasoning Deterministic checks and scripts"},{"location":"cron/#parameters","title":"Parameters","text":"<ul> <li><code>exec_command</code> \u2014 The shell command to run (mutually exclusive with <code>message</code>)</li> <li><code>PREV_OUTPUT</code> \u2014 Environment variable containing the previous run's stdout, enabling change detection</li> </ul>"},{"location":"cron/#execution-flow","title":"Execution flow","text":"<pre><code>graph LR\n    TIMER[Timer fires] --&gt; EXEC[Run command in sandbox]\n    EXEC --&gt; CHECK{Output empty?}\n    CHECK --&gt;|no| DELIVER[Deliver to channel]\n    CHECK --&gt;|yes| SKIP[Skip silently]\n\n    style TIMER fill:#ab47bc,stroke:#8e24aa,color:#fff\n    style EXEC fill:#5c6bc0,stroke:#3949ab,color:#fff\n    style CHECK fill:#7c4dff,stroke:#651fff,color:#fff\n    style DELIVER fill:#ffa726,stroke:#fb8c00,color:#fff\n    style SKIP fill:#78909c,stroke:#546e7a,color:#fff</code></pre> <p>Exec commands run inside the sandbox (bubblewrap or Docker) by default. Set <code>sandbox: none</code> to run directly on the host.</p>"},{"location":"cron/#examples","title":"Examples","text":"<p>Air quality check (only notify on change):</p> <pre><code>autobot cron add \\\n  --name \"air-quality\" \\\n  --exec 'curl -s \"https://api.example.com/aqi?city=NYC\" | jq -r .aqi' \\\n  --every 3600 \\\n  --channel telegram --to \"12345\"\n</code></pre> <p>The <code>PREV_OUTPUT</code> variable lets you detect changes:</p> <pre><code>autobot cron add \\\n  --name \"uptime-monitor\" \\\n  --exec 'STATUS=$(curl -so /dev/null -w \"%{http_code}\" https://example.com); [ \"$STATUS\" != \"$PREV_OUTPUT\" ] &amp;&amp; echo \"$STATUS\" || true' \\\n  --every 300 \\\n  --channel slack --to \"C12345\"\n</code></pre> <p>Disk space alert:</p> <pre><code>autobot cron add \\\n  --name \"disk-check\" \\\n  --exec 'USAGE=$(df -h / | awk \"NR==2{print \\$5}\" | tr -d \"%\"); [ \"$USAGE\" -gt 90 ] &amp;&amp; echo \"Disk usage: ${USAGE}%\" || true' \\\n  --cron \"0 */6 * * *\" \\\n  --channel telegram --to \"12345\"\n</code></pre>"},{"location":"cron/#configuration","title":"Configuration","text":""},{"location":"cron/#enable-cron","title":"Enable Cron","text":"<pre><code>cron:\n  enabled: true\n  store_path: \"./cron.json\"  # Optional, defaults to workspace\n</code></pre>"},{"location":"cron/#agent-creates-jobs","title":"Agent Creates Jobs","text":"<p>The agent creates cron jobs via the <code>cron</code> tool when users make scheduling requests. The tool supports three actions:</p> Action Description Required Parameters <code>add</code> Create a new job <code>message</code> + one of: <code>every_seconds</code>, <code>cron_expr</code>, <code>at</code> <code>list</code> List jobs for current owner \u2014 <code>show</code> Show full job details <code>job_id</code> <code>update</code> Update schedule or message in-place <code>job_id</code> + at least one of: <code>message</code>, <code>every_seconds</code>, <code>cron_expr</code>, <code>at</code> <code>enable</code> Resume a paused job <code>job_id</code> <code>disable</code> Pause a job without deleting it <code>job_id</code> <code>remove</code> Delete a job <code>job_id</code>"},{"location":"cron/#telegram-cron-command","title":"Telegram <code>/cron</code> Command","text":"<p>Send <code>/cron</code> in Telegram to instantly see all your scheduled jobs \u2014 no LLM round-trip needed.</p> <p>Example output:</p> <pre><code>Scheduled jobs (2)\n\n1. abc123 \u2014 Check GitHub stars\n   \u23f1 Every 10 min | \u2705 2 min ago\n\n2. def456 \u2014 Morning briefing\n   \ud83d\udd50 0 9 * * 1-5 (UTC) | \u23f3 pending\n</code></pre> <p>Empty state shows: \"No scheduled jobs. Ask me in chat to schedule something.\"</p>"},{"location":"cron/#built-in-cron-skill","title":"Built-in Cron Skill","text":"<p>A built-in skill (<code>src/skills/cron/SKILL.md</code>) is automatically loaded when the cron tool is registered. It teaches the agent:</p> <ul> <li>Message quality \u2014 write self-contained prompts with specific tool names and URLs</li> <li>Timezone handling \u2014 ask the user, convert to UTC, confirm both times</li> <li>Update-first rule \u2014 list existing jobs before creating, update instead of duplicate</li> <li>Schedule type selection \u2014 when to use <code>every_seconds</code> vs <code>cron_expr</code> vs <code>at</code></li> </ul> <p>The skill declares <code>tool: cron</code> in its frontmatter, so it's included automatically whenever the cron tool is available \u2014 both in interactive and background turns.</p>"},{"location":"cron/#cli-management","title":"CLI Management","text":""},{"location":"cron/#list-jobs","title":"List Jobs","text":"<pre><code>autobot cron list        # Show enabled jobs\nautobot cron list --all  # Include disabled jobs\n</code></pre>"},{"location":"cron/#show-job-details","title":"Show Job Details","text":"<pre><code>autobot cron show &lt;job_id&gt;\n</code></pre>"},{"location":"cron/#add-jobs-manually","title":"Add Jobs Manually","text":"<pre><code># Recurring interval\nautobot cron add --name \"standup\" --message \"Time for standup!\" --every 3600\n\n# Cron expression\nautobot cron add --name \"morning\" --message \"Good morning!\" --cron \"0 9 * * *\"\n\n# One-time\nautobot cron add --name \"reminder\" --message \"Call dentist\" --at \"2026-02-20T15:00:00Z\"\n</code></pre>"},{"location":"cron/#update-a-job","title":"Update a Job","text":"<pre><code># Change schedule\nautobot cron update &lt;job_id&gt; --cron \"0 8 * * *\"\n\n# Change message\nautobot cron update &lt;job_id&gt; --message \"New task instructions\"\n\n# Change both\nautobot cron update &lt;job_id&gt; --every 600 --message \"Updated check\"\n</code></pre>"},{"location":"cron/#remove-a-job","title":"Remove a Job","text":"<pre><code>autobot cron remove &lt;job_id&gt;\n</code></pre>"},{"location":"cron/#clear-all-jobs","title":"Clear All Jobs","text":"<pre><code>autobot cron clear\n</code></pre>"},{"location":"cron/#enabledisable","title":"Enable/Disable","text":"<pre><code>autobot cron enable &lt;job_id&gt;\nautobot cron disable &lt;job_id&gt;\n</code></pre>"},{"location":"cron/#force-run","title":"Force Run","text":"<pre><code>autobot cron run &lt;job_id&gt;          # Run if enabled\nautobot cron run &lt;job_id&gt; --force  # Run even if disabled\n</code></pre>"},{"location":"cron/#per-owner-isolation","title":"Per-Owner Isolation","text":"<p>Jobs created via the <code>cron</code> tool are automatically scoped to the originating channel and chat. A Telegram user's jobs are isolated from a Slack user's jobs.</p> <ul> <li>Owner format: <code>channel:chat_id</code> (e.g., <code>telegram:634643933</code>)</li> <li>List only shows the current owner's jobs</li> <li>Update only works on the current owner's jobs</li> <li>Remove only works on the current owner's jobs</li> <li>Jobs created via CLI have no owner restriction</li> </ul>"},{"location":"cron/#examples_1","title":"Examples","text":""},{"location":"cron/#one-time-reminder","title":"One-Time Reminder","text":"<p>User says: \"Remind me in 30 minutes to take a break\"</p> <p>The agent creates: <pre><code>cron add:\n  message: \"Send the user a reminder to take a break.\"\n  at: \"2026-02-20T10:30:00Z\"\n</code></pre></p> <p>Job fires once, delivers the reminder, then auto-deletes.</p>"},{"location":"cron/#daily-report","title":"Daily Report","text":"<p>User says: \"Send me a weather summary every morning at 9am\"</p> <p>The agent creates: <pre><code>cron add:\n  message: \"Use web_search to find current weather for user's location.\n            Send a brief summary.\"\n  cron_expr: \"0 9 * * *\"\n</code></pre></p>"},{"location":"cron/#periodic-check","title":"Periodic Check","text":"<p>User says: \"Check my email every 5 minutes\"</p> <p>The agent creates: <pre><code>cron add:\n  message: \"Check email and notify the user of new messages.\"\n  every_seconds: 300\n</code></pre></p>"},{"location":"cron/#update-existing-job","title":"Update Existing Job","text":"<p>User says: \"Change my morning report to 8am instead\"</p> <p>The agent updates: <pre><code>cron update:\n  job_id: \"a1b2c3d4\"\n  cron_expr: \"0 8 * * *\"\n</code></pre></p> <p>Job identity (<code>id</code>, <code>created_at_ms</code>, <code>state</code>) is preserved \u2014 only the schedule changes.</p>"},{"location":"cron/#file-structure","title":"File Structure","text":"<pre><code>workspace/\n\u2514\u2500\u2500 cron.json    # Job definitions (auto-created)\n</code></pre> <p>Permissions:</p> <ul> <li><code>cron.json</code>: <code>0600</code> (user read/write only)</li> <li>Parent directory: <code>0700</code> (user-only access)</li> </ul>"},{"location":"cron/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cron/#job-doesnt-fire","title":"Job doesn't fire","text":"<p>Check:</p> <ol> <li>Is cron enabled in config? (<code>cron.enabled: true</code>)</li> <li>Is the job enabled? (<code>autobot cron show &lt;job_id&gt;</code>)</li> <li>Is the gateway running? (Jobs only fire while the gateway process is active)</li> <li>Check logs for <code>Cron: executing job</code> entries</li> </ol>"},{"location":"cron/#see-also","title":"See Also","text":"<ul> <li>Architecture \u2014 System design and message flow</li> <li>Configuration \u2014 Full config reference</li> <li>Security \u2014 File permissions and job isolation</li> </ul>"},{"location":"deepseek/","title":"DeepSeek","text":"<p>Autobot supports DeepSeek as an LLM provider via its OpenAI-compatible API. This gives access to DeepSeek-V3, DeepSeek-R1, and other DeepSeek models known for strong reasoning and coding performance at competitive pricing.</p>"},{"location":"deepseek/#setup","title":"Setup","text":""},{"location":"deepseek/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at platform.deepseek.com.</p>"},{"location":"deepseek/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>DEEPSEEK_API_KEY=...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"DeepSeek\" as provider\n</code></pre>"},{"location":"deepseek/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"deepseek/deepseek-chat\"\n\nproviders:\n  deepseek:\n    api_key: \"${DEEPSEEK_API_KEY}\"\n</code></pre>"},{"location":"deepseek/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (deepseek)\n</code></pre>"},{"location":"deepseek/#model-naming","title":"Model naming","text":"<p>Models use the <code>deepseek/</code> prefix followed by the DeepSeek model ID:</p> <pre><code># General-purpose (DeepSeek-V3)\nmodel: \"deepseek/deepseek-chat\"\n\n# Reasoning (DeepSeek-R1)\nmodel: \"deepseek/deepseek-reasoner\"\n</code></pre> <p>The <code>deepseek/</code> prefix tells autobot to route to the DeepSeek API. It is stripped before sending to the API.</p>"},{"location":"deepseek/#reasoning-content","title":"Reasoning content","text":"<p>DeepSeek-R1 (the <code>deepseek-reasoner</code> model) returns reasoning traces in its responses. Autobot captures these in the <code>reasoning_content</code> field, allowing downstream features to access the model's chain-of-thought.</p>"},{"location":"deepseek/#custom-api-base","title":"Custom API base","text":"<p>To use a proxy or alternative endpoint:</p> <pre><code>providers:\n  deepseek:\n    api_key: \"${DEEPSEEK_API_KEY}\"\n    api_base: \"https://your-proxy.example.com/v1/chat/completions\"\n</code></pre>"},{"location":"deepseek/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 DeepSeek API key <code>api_base</code> No <code>https://api.deepseek.com/v1/chat/completions</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"deepseek/#how-it-works","title":"How it works","text":"<p>DeepSeek uses the OpenAI-compatible Chat Completions API format:</p> <ul> <li><code>Authorization: Bearer</code> header for authentication</li> <li>Standard message format with <code>role</code> and <code>content</code> fields</li> <li><code>reasoning_content</code> field returned for reasoning models (DeepSeek-R1)</li> </ul> <p>Autobot detects DeepSeek models by the <code>deepseek</code> keyword and routes to the correct endpoint automatically. Tools, MCP servers, plugins, and all other features work the same as with other providers.</p>"},{"location":"deepseek/#voice-transcription","title":"Voice transcription","text":"<p>DeepSeek does not provide a transcription API. If you need voice message support, configure an additional Groq or OpenAI provider for Whisper-based transcription.</p>"},{"location":"deepseek/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> </ul>"},{"location":"deepseek/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://api.deepseek.com/v1/chat/completions model=...</code> \u2014 confirms provider is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"deepseek/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: authentication failed\" \u2014 Invalid or expired API key. Verify at platform.deepseek.com.</p> <p>\"API error: rate limit exceeded\" \u2014 Too many requests. DeepSeek applies per-key rate limits based on your plan.</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>Security-first deployment for Autobot with production-ready examples.</p>"},{"location":"deployment/#system-requirements","title":"System Requirements","text":"<p>Sandboxing (required for workspace restrictions):</p> <ul> <li> <p>bubblewrap (recommended for development/Pi):   <pre><code># Ubuntu/Debian\nsudo apt install bubblewrap\n\n# Fedora\nsudo dnf install bubblewrap\n\n# Arch\nsudo pacman -S bubblewrap\n</code></pre></p> </li> <li> <p>Docker (recommended for production):   <pre><code># Ubuntu/Debian\nsudo apt install docker.io\n\n# Others: https://docs.docker.com/engine/install/\n</code></pre></p> </li> </ul> <p>Note: Autobot will fail to start if sandboxing is enabled (<code>tools.sandbox: auto/bubblewrap/docker</code>) but no sandbox tool is available.</p>"},{"location":"deployment/#quick-start-local","title":"Quick Start (Local)","text":"<p>Create a new bot in seconds:</p> <pre><code>autobot new optimus\ncd optimus\n</code></pre> <p>Install bubblewrap (required for workspace restrictions): <pre><code>sudo apt install bubblewrap  # Ubuntu/Debian\n</code></pre></p> <p>Edit <code>.env</code> and add your API keys: <pre><code>vi .env  # Add ANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Validate and start: <pre><code>autobot doctor    # Check for issues (validates sandbox availability)\nautobot gateway   # Start gateway\n</code></pre></p> <p>\u2713 Secure by default: Kernel-enforced workspace sandbox, localhost binding, .env protection, shell safety.</p>"},{"location":"deployment/#multiple-bots-one-machine","title":"Multiple Bots (One Machine)","text":"<p>Run multiple isolated bots:</p> <pre><code># Personal bot\nautobot new personal-bot\ncd personal-bot &amp;&amp; autobot gateway --port 18790\n\n# Work bot\ncd .. &amp;&amp; autobot new work-bot\ncd work-bot &amp;&amp; autobot gateway --port 18791\n</code></pre> <p>Each bot has isolated config, workspace, sessions, and logs.</p>"},{"location":"deployment/#production-deployment","title":"Production Deployment","text":""},{"location":"deployment/#docker-recommended","title":"Docker (Recommended)","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  autobot:\n    image: autobot:latest\n    container_name: autobot-prod\n    user: \"1000:1000\"\n    read_only: true\n    security_opt:\n      - no-new-privileges:true\n    env_file:\n      - .env\n    volumes:\n      - ./config.yml:/app/config.yml:ro\n      - autobot-workspace:/app/workspace\n      - autobot-sessions:/app/sessions\n      # Docker socket (for Docker-in-Docker sandbox)\n      - /var/run/docker.sock:/var/run/docker.sock\n    tmpfs:\n      - /tmp\n    ports:\n      - \"127.0.0.1:18790:18790\"\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 2G\n        reservations:\n          cpus: '0.5'\n          memory: 512M\n\nvolumes:\n  autobot-workspace:\n  autobot-sessions:\n</code></pre> <p>Configuration for Docker deployment: <pre><code>tools:\n  sandbox: \"docker\"  # Use Docker for sandboxing (auto-detects if not specified)\n</code></pre></p> <p>Start: <pre><code>docker-compose up -d\ndocker-compose logs -f autobot\n</code></pre></p> <p>Security enabled: Non-root user, read-only filesystem, no new privileges, localhost binding, resource limits, Docker-based sandboxing.</p>"},{"location":"deployment/#systemd-linux","title":"Systemd (Linux)","text":"<p>Use the provided service template:</p> <pre><code># Copy service file\nsudo cp docs/templates/autobot.service /etc/systemd/system/\n\n# Build and install binary\nmake release\nsudo cp bin/autobot /usr/local/bin/\nsudo chmod 755 /usr/local/bin/autobot\n\n# Enable service\nsudo systemctl daemon-reload\nsudo systemctl enable --now autobot\n</code></pre> <p>Check status: <pre><code>sudo systemctl status autobot\nsudo journalctl -u autobot -f\n</code></pre></p> <p>Service includes: Security hardening (NoNewPrivileges, ProtectSystem), resource limits, automatic restarts.</p>"},{"location":"deployment/#configuration-validation","title":"Configuration Validation","text":"<p>Always validate before deploying:</p> <pre><code>autobot doctor          # Check for errors/warnings\nautobot doctor --strict # Fail on any warning (CI/CD)\n</code></pre>"},{"location":"deployment/#what-it-checks","title":"What It Checks","text":"<p>\u274c Errors (blocks deployment):</p> <ul> <li>Sandbox enabled but not available</li> <li>Plaintext secrets in <code>config.yml</code></li> <li><code>.env</code> permissions (must be 0600)</li> <li><code>.env</code> inside workspace (exposes secrets)</li> <li>No LLM provider configured</li> </ul> <p>\u26a0\ufe0f Warnings (review recommended):</p> <ul> <li>Gateway bound to 0.0.0.0 (network exposure)</li> <li>Empty channel allowlists</li> <li>Missing <code>.env</code> file</li> <li>Workspace restrictions disabled</li> </ul>"},{"location":"deployment/#external-access-optional","title":"External Access (Optional)","text":""},{"location":"deployment/#reverse-proxy-nginx","title":"Reverse Proxy (Nginx)","text":"<p>For external access with TLS, use a reverse proxy. Example configuration in <code>docs/templates/nginx.conf</code>:</p> <pre><code># Install nginx\nsudo apt install nginx  # Ubuntu/Debian\n\n# Configure proxy (see docs/templates/nginx.conf)\nsudo cp docs/templates/nginx.conf /etc/nginx/sites-available/autobot\nsudo ln -s /etc/nginx/sites-available/autobot /etc/nginx/sites-enabled/\n\n# Get TLS certificate\nsudo apt install certbot python3-certbot-nginx\nsudo certbot --nginx -d autobot.example.com\n\n# Enable\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre> <p>Key features (from template):</p> <ul> <li>TLS 1.2+ with strong ciphers</li> <li>Security headers (HSTS, X-Frame-Options, CSP)</li> <li>WebSocket support</li> <li>Request buffering disabled for streaming</li> <li>Optional basic auth</li> </ul>"},{"location":"deployment/#security-best-practices","title":"Security Best Practices","text":""},{"location":"deployment/#1-secrets-management","title":"1. Secrets Management","text":"<pre><code># .gitignore (auto-created by autobot new)\n.env\n.env.*\nsessions/\nlogs/\nworkspace/memory/\n\n# Verify .env permissions\nls -l .env\n# Should show: -rw------- (0600)\n</code></pre>"},{"location":"deployment/#2-workspace-restrictions","title":"2. Workspace Restrictions","text":"<pre><code>tools:\n  sandbox: auto  # \u2713 Default (auto-detect bubblewrap or Docker)\n</code></pre>"},{"location":"deployment/#3-channel-authorization","title":"3. Channel Authorization","text":"<pre><code>channels:\n  telegram:\n    enabled: true\n    allow_from: [\"123456789\"]  # Specific user IDs\n    # NEVER: [\"*\"]             # Allows anyone!\n</code></pre>"},{"location":"deployment/#4-network-binding","title":"4. Network Binding","text":"<pre><code>gateway:\n  host: \"127.0.0.1\"  # \u2713 Default (localhost only)\n  # AVOID: \"0.0.0.0\" # Exposes to all interfaces\n</code></pre> <p>Use a reverse proxy (nginx) for external access.</p>"},{"location":"deployment/#5-file-permissions","title":"5. File Permissions","text":"<pre><code># Recommended permissions\n/var/lib/autobot/         0700 (autobot:autobot)\n/var/lib/autobot/.env     0600 (autobot:autobot)\n/var/lib/autobot/config.yml 0600 (autobot:autobot)\n</code></pre>"},{"location":"deployment/#advanced-setup","title":"Advanced Setup","text":""},{"location":"deployment/#dedicated-user-account-production","title":"Dedicated User Account (Production)","text":"<p>Create a system user for Autobot:</p> <pre><code># Linux\nsudo useradd -r -m -d /var/lib/autobot -s /bin/bash autobot\nsudo -u autobot autobot new optimus\n\n# macOS\nsudo dscl . -create /Users/autobot\nsudo dscl . -create /Users/autobot NFSHomeDirectory /var/lib/autobot\nsudo mkdir -p /var/lib/autobot\nsudo chown autobot:staff /var/lib/autobot\n</code></pre>"},{"location":"deployment/#resource-limits-systemd","title":"Resource Limits (Systemd)","text":"<p>Add to <code>autobot.service</code>:</p> <pre><code>[Service]\nCPUQuota=200%      # Max 2 cores\nMemoryMax=2G       # Hard limit\nMemoryHigh=1.5G    # Soft limit\nLimitNOFILE=65536  # File descriptors\nTasksMax=512       # Max processes\n</code></pre>"},{"location":"deployment/#backup","title":"Backup","text":"<p>Essential files to backup: <pre><code>/var/lib/autobot/.env\n/var/lib/autobot/config.yml\n/var/lib/autobot/sessions/\n</code></pre></p> <p>Simple backup script: <pre><code>#!/bin/bash\nBACKUP_DIR=\"/backup/autobot/$(date +%Y%m%d)\"\nmkdir -p \"$BACKUP_DIR\"\ntar czf \"$BACKUP_DIR/autobot-backup.tar.gz\" \\\n  -C /var/lib/autobot \\\n  .env config.yml sessions/\n</code></pre></p>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#config-validation-fails","title":"Config validation fails","text":"<pre><code>autobot doctor  # Shows specific errors and fixes\n</code></pre>"},{"location":"deployment/#gateway-wont-start","title":"Gateway won't start","text":"<pre><code>autobot doctor --strict  # Check for warnings\nsudo systemctl status autobot  # If using systemd\nsudo journalctl -u autobot -f  # View logs\n</code></pre>"},{"location":"deployment/#secrets-not-loading","title":"Secrets not loading","text":"<pre><code># Verify .env exists and has correct permissions\nls -la .env\nchmod 600 .env\n\n# Check environment variable syntax in config.yml\ngrep ANTHROPIC_API_KEY config.yml\n# Should show: api_key: \"${ANTHROPIC_API_KEY}\"\n</code></pre>"},{"location":"deployment/#permission-denied","title":"Permission denied","text":"<pre><code># Verify ownership (if using dedicated user)\nls -la /var/lib/autobot/\nsudo chown -R autobot:autobot /var/lib/autobot\n</code></pre>"},{"location":"deployment/#nginx-502-bad-gateway","title":"Nginx 502 Bad Gateway","text":"<pre><code># Check if autobot is running\nsudo systemctl status autobot\ncurl http://127.0.0.1:18790/health\n\n# Check nginx logs\nsudo tail -f /var/log/nginx/autobot_error.log\n</code></pre>"},{"location":"deployment/#quick-reference","title":"Quick Reference","text":""},{"location":"deployment/#commands","title":"Commands","text":"<pre><code>autobot new optimus              # Create new bot\nautobot doctor                   # Validate config\nautobot doctor --strict          # Strict validation\nautobot gateway                  # Start gateway\nautobot gateway --port 8080      # Custom port\n</code></pre>"},{"location":"deployment/#files","title":"Files","text":"<ul> <li><code>.env</code> - Secrets (never commit, 0600)</li> <li><code>config.yml</code> - Configuration (uses ${ENV_VARS})</li> <li><code>workspace/</code> - Sandboxed LLM workspace</li> <li><code>sessions/</code> - Conversation history</li> <li><code>docs/templates/</code> - Production templates (systemd, nginx, docker)</li> </ul>"},{"location":"deployment/#security-defaults","title":"Security Defaults","text":"<ul> <li>\u2713 Workspace sandbox enabled</li> <li>\u2713 .env files blocked from LLM</li> <li>\u2713 Symlink operations blocked</li> <li>\u2713 Localhost-only binding</li> <li>\u2713 Destructive commands blocked</li> <li>\u2713 File permissions enforced</li> <li>\u2713 Validation on startup</li> </ul>"},{"location":"deployment/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production:</p> <ul> <li>[ ] <code>autobot doctor --strict</code> passes</li> <li>[ ] Dedicated user account (not root)</li> <li>[ ] .env file permissions (0600)</li> <li>[ ] .env not in workspace directory</li> <li>[ ] TLS configured (Let's Encrypt)</li> <li>[ ] Gateway bound to localhost</li> <li>[ ] Nginx reverse proxy (if external access)</li> <li>[ ] Resource limits configured</li> <li>[ ] Backup script setup</li> <li>[ ] Log monitoring enabled</li> <li>[ ] Firewall rules configured</li> </ul>"},{"location":"deployment/#learn-more","title":"Learn More","text":"<ul> <li>Security - Security model and threat mitigations</li> <li>Configuration - Full configuration reference</li> <li>Architecture - System design and components</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Crystal 1.10+</li> <li>Shards</li> <li>Make</li> </ul>"},{"location":"development/#build","title":"Build","text":"<pre><code>make deps\nmake build      # debug binary\nmake release    # optimized binary\nmake static     # static binary (Linux/musl)\n</code></pre>"},{"location":"development/#test","title":"Test","text":"<pre><code>make test\nmake test-verbose\ncrystal spec spec/autobot/tools/filesystem_spec.cr\n</code></pre>"},{"location":"development/#quality","title":"Quality","text":"<pre><code>make format\nmake format-check\nmake lint\n</code></pre>"},{"location":"development/#docker","title":"Docker","text":"<pre><code>make docker-build\nmake docker-run\nmake docker-shell\nmake docker-size\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Preview the docs site locally:</p> <pre><code>pip install 'mkdocs&lt;2' mkdocs-material\nmkdocs serve\n</code></pre> <p>This starts a dev server at <code>http://127.0.0.1:8000/autobot/</code> with live reload \u2014 any changes to <code>docs/</code> or <code>mkdocs.yml</code> are reflected immediately.</p>"},{"location":"development/#release-artifacts","title":"Release Artifacts","text":"<pre><code>make release-all\nmake checksums\n</code></pre>"},{"location":"gemini/","title":"Google Gemini","text":"<p>Autobot supports Google Gemini as an LLM provider via the OpenAI-compatible endpoint. This gives access to Gemini Pro, Flash, and other Google AI models.</p>"},{"location":"gemini/#setup","title":"Setup","text":""},{"location":"gemini/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at aistudio.google.com.</p>"},{"location":"gemini/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>GEMINI_API_KEY=...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"Google Gemini\" as provider\n</code></pre>"},{"location":"gemini/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"gemini/gemini-2.5-flash\"\n\nproviders:\n  gemini:\n    api_key: \"${GEMINI_API_KEY}\"\n</code></pre>"},{"location":"gemini/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (gemini)\n</code></pre>"},{"location":"gemini/#model-naming","title":"Model naming","text":"<p>Models use the <code>gemini/</code> prefix followed by the Google model ID:</p> <pre><code># Gemini 2.5\nmodel: \"gemini/gemini-2.5-pro\"\nmodel: \"gemini/gemini-2.5-flash\"\n\n# Gemini 2.0\nmodel: \"gemini/gemini-2.0-flash\"\n\n# Gemini 1.5\nmodel: \"gemini/gemini-1.5-pro\"\nmodel: \"gemini/gemini-1.5-flash\"\n</code></pre> <p>The <code>gemini/</code> prefix tells autobot to route to the Gemini API. It is stripped before sending to the API.</p> <p>See the full model list in the Gemini docs.</p>"},{"location":"gemini/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 Google AI API key <code>api_base</code> No <code>https://generativelanguage.googleapis.com/v1beta/openai/chat/completions</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"gemini/#how-it-works","title":"How it works","text":"<p>Gemini uses Google's OpenAI-compatible endpoint, which follows the standard Chat Completions format:</p> <ul> <li><code>Authorization: Bearer</code> header for authentication</li> <li>Standard message format with <code>role</code> and <code>content</code> fields</li> <li>Function calling via <code>tools</code> array</li> </ul> <p>Autobot detects Gemini models by the <code>gemini</code> keyword and routes to Google's OpenAI-compatible endpoint automatically. Tools, MCP servers, plugins, and all other features work the same as with other providers.</p> <p>Note that Gemini's error responses may use an array-wrapped format (<code>[{\"error\": {...}}]</code>). Autobot handles both standard and array-wrapped error formats transparently.</p>"},{"location":"gemini/#voice-transcription","title":"Voice transcription","text":"<p>Gemini does not provide a Whisper-compatible transcription API. If you need voice message support, configure an additional Groq or OpenAI provider for Whisper-based transcription.</p>"},{"location":"gemini/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> <li>Free tier limits \u2014 Google AI Studio has per-minute and per-day request limits on the free tier.</li> </ul>"},{"location":"gemini/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://generativelanguage.googleapis.com/... model=...</code> \u2014 confirms provider is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"gemini/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: API key not valid\" \u2014 Invalid or expired API key. Verify at aistudio.google.com.</p> <p>\"API error: Resource has been exhausted\" \u2014 Rate limit or quota exceeded. Check your usage and limits in Google AI Studio.</p> <p>\"API error: model not found\" \u2014 Model ID is wrong or not available. Check the Gemini models page for current availability.</p>"},{"location":"groq/","title":"Groq","text":"<p>Autobot supports Groq as an LLM provider via its OpenAI-compatible API. Groq is known for ultra-fast inference powered by custom LPU hardware, offering models like Llama, Mixtral, and Gemma with extremely low latency.</p>"},{"location":"groq/#setup","title":"Setup","text":""},{"location":"groq/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at console.groq.com.</p>"},{"location":"groq/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>GROQ_API_KEY=gsk_...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"Groq\" as provider\n</code></pre>"},{"location":"groq/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"groq/llama-3.3-70b-versatile\"\n\nproviders:\n  groq:\n    api_key: \"${GROQ_API_KEY}\"\n</code></pre>"},{"location":"groq/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (groq)\n</code></pre>"},{"location":"groq/#model-naming","title":"Model naming","text":"<p>Models use the <code>groq/</code> prefix followed by the Groq model ID:</p> <pre><code># Llama models\nmodel: \"groq/llama-3.3-70b-versatile\"\nmodel: \"groq/llama-3.1-8b-instant\"\n\n# Mixtral\nmodel: \"groq/mixtral-8x7b-32768\"\n\n# Gemma\nmodel: \"groq/gemma2-9b-it\"\n</code></pre> <p>The <code>groq/</code> prefix tells autobot to route to the Groq API. It is stripped before sending to the API.</p> <p>See the full model list in the Groq docs.</p>"},{"location":"groq/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 Groq API key (<code>gsk_...</code>) <code>api_base</code> No <code>https://api.groq.com/openai/v1/chat/completions</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"groq/#how-it-works","title":"How it works","text":"<p>Groq uses the OpenAI-compatible Chat Completions API format:</p> <ul> <li><code>Authorization: Bearer</code> header for authentication</li> <li>Standard message format with <code>role</code> and <code>content</code> fields</li> <li>Function calling via <code>tools</code> array</li> </ul> <p>Autobot detects Groq models by the <code>groq</code> keyword and routes to the correct endpoint automatically. Tools, MCP servers, plugins, and all other features work the same as with other providers.</p>"},{"location":"groq/#voice-transcription","title":"Voice transcription","text":"<p>Groq provides the Whisper API for voice transcription. When Groq is configured, voice messages are automatically transcribed using <code>whisper-large-v3-turbo</code>. No extra configuration is needed \u2014 the API key is reused from the provider config.</p> <p>Groq is the preferred transcription provider when available (faster than OpenAI, free tier included). If both Groq and OpenAI are configured, Groq takes priority for transcription.</p>"},{"location":"groq/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> <li>Rate limits \u2014 Groq's free tier has token and request limits. Check console.groq.com for current limits.</li> </ul>"},{"location":"groq/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://api.groq.com/openai/v1/chat/completions model=...</code> \u2014 confirms provider is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"groq/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: Invalid API Key\" \u2014 Invalid or revoked API key. Verify at console.groq.com.</p> <p>\"API error: Rate limit reached\" \u2014 Too many requests or tokens per minute. Groq's free tier has lower limits \u2014 consider upgrading or spacing out requests.</p> <p>\"API error: model_not_found\" \u2014 Model ID is wrong or has been deprecated. Check the Groq models page for current availability.</p>"},{"location":"mcp/","title":"MCP (Model Context Protocol)","text":"<p>Autobot can connect to external MCP servers to extend the LLM's capabilities with remote tools. MCP servers expose tools via JSON-RPC 2.0 over stdio \u2014 autobot discovers them at startup and registers them as regular tools so the LLM can use them transparently.</p>"},{"location":"mcp/#how-it-works","title":"How It Works","text":"<ol> <li>On startup, autobot reads the <code>mcp.servers</code> section from <code>config.yml</code></li> <li>All servers are started in the background \u2014 they do not block startup</li> <li>For each server, it spawns the command as a child process</li> <li>Performs the MCP protocol handshake (initialize + notifications/initialized)</li> <li>Calls <code>tools/list</code> to discover available tools</li> <li>Registers each tool as <code>mcp_{server}_{tool}</code> in the tool registry</li> <li>The LLM sees these as native tools and can call them like any other</li> </ol> <p>Since MCP servers connect asynchronously, tools become available shortly after startup rather than delaying it. Multiple servers are started concurrently.</p>"},{"location":"mcp/#configuration","title":"Configuration","text":"<p>Add an <code>mcp</code> section to your <code>config.yml</code>:</p> <pre><code>mcp:\n  servers:\n    garmin:\n      command: \"uvx\"\n      args: [\"--python\", \"3.12\", \"--from\", \"git+https://github.com/Taxuspt/garmin_mcp\", \"garmin-mcp\"]\n      env:\n        GARMIN_EMAIL: \"${GARMIN_EMAIL}\"\n        GARMIN_PASSWORD: \"${GARMIN_PASSWORD}\"\n    github:\n      command: \"npx\"\n      args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n      env:\n        GITHUB_TOKEN: \"${GITHUB_TOKEN}\"\n</code></pre> <p>Each server entry has:</p> Field Type Description <code>command</code> string Executable to spawn (must be in PATH) <code>args</code> string[] Command-line arguments <code>env</code> map Environment variables passed to the process <code>tools</code> string[] Tool allowlist (empty = all). Supports <code>*</code> prefix matching <p>Environment variables support <code>${VAR}</code> expansion from your <code>.env</code> file or shell environment.</p>"},{"location":"mcp/#tool-naming","title":"Tool Naming","text":"<p>MCP tools are prefixed to avoid collisions with built-in tools:</p> <pre><code>mcp_{server_name}_{tool_name}\n</code></pre> <p>All characters outside <code>[a-z0-9_]</code> are replaced with underscores. For example:</p> Server Remote Tool Registered As garmin get_activities <code>mcp_garmin_get_activities</code> github list-repos <code>mcp_github_list_repos</code>"},{"location":"mcp/#security","title":"Security","text":"<p>MCP servers run as regular child processes (not sandboxed) because they typically need network access for external APIs. However, several safeguards are in place:</p> <ul> <li>Env isolation: Only explicitly configured env vars are passed to the process, plus <code>PATH</code>, <code>HOME</code>, and <code>LANG</code> from the host</li> <li>No workspace sharing: MCP processes do not receive access to autobot's workspace directory</li> <li>Timeouts: 30s for initialization handshake, 60s for tool calls</li> <li>Response truncation: Tool results are capped at 50KB to prevent memory issues</li> <li>No auto-restart: If a server crashes, its tools return errors until autobot is restarted</li> </ul>"},{"location":"mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/#server-fails-to-start","title":"Server fails to start","text":"<p>Check that the command is installed and in your PATH:</p> <pre><code>which uvx   # or npx, or whatever command you configured\n</code></pre> <p>Enable debug logging to see stderr output from MCP servers:</p> <pre><code>LOG_LEVEL=debug autobot agent -m \"test\"\n</code></pre>"},{"location":"mcp/#tools-not-discovered","title":"Tools not discovered","text":"<p>Verify the server responds to the MCP protocol. You can test manually:</p> <pre><code>echo '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2025-03-26\",\"capabilities\":{},\"clientInfo\":{\"name\":\"test\",\"version\":\"0.1.0\"}}}' | npx -y @modelcontextprotocol/server-github\n</code></pre>"},{"location":"mcp/#timeouts","title":"Timeouts","text":"<p>If a server takes longer than 30s to initialize (e.g. downloading dependencies on first run), the handshake will fail. Pre-install dependencies before running autobot:</p> <pre><code># For Python-based servers\nuvx --python 3.12 --from git+https://... garmin-mcp --help\n\n# For Node-based servers\nnpx -y @modelcontextprotocol/server-github --help\n</code></pre>"},{"location":"media/","title":"Media support","text":"<p>Autobot handles media in three directions:</p> <ul> <li>Vision (inbound) \u2014 photos sent by users are downloaded, base64-encoded, and forwarded to the LLM as multimodal content blocks</li> <li>Image generation (outbound) \u2014 the LLM can create images via the <code>generate_image</code> tool and send them back to users</li> <li>Voice transcription (inbound) \u2014 voice messages are transcribed to text via the Whisper API before reaching the LLM</li> </ul>"},{"location":"media/#vision","title":"Vision","text":""},{"location":"media/#how-it-works","title":"How it works","text":"<pre><code>Channel (Telegram) -&gt; Download &amp; base64 encode -&gt; Context builder -&gt; LLM provider\n</code></pre> <ol> <li>Channel receives a photo and downloads the file bytes via the platform API</li> <li>MediaAttachment stores the base64-encoded data in a transient <code>data</code> field (excluded from JSON serialization to avoid bloating session files)</li> <li>Context builder detects attachments with <code>data</code> and builds an array of content blocks (text + image) in OpenAI's <code>image_url</code> format</li> <li>Provider sends the content blocks directly for OpenAI-compatible APIs, or converts them to Anthropic's <code>image/source/base64</code> format for the native Anthropic path</li> </ol>"},{"location":"media/#supported-channels","title":"Supported channels","text":"Channel Status Notes Telegram Supported Auto-downloads photos via Bot API Slack Planned Needs <code>url_private</code> download with auth WhatsApp Planned Needs bridge-side changes to forward images Zulip Not supported Media handling not yet implemented"},{"location":"media/#supported-providers","title":"Supported providers","text":"<p>All providers work with vision \u2014 the internal format uses OpenAI-compatible <code>image_url</code> content blocks:</p> <ul> <li>OpenAI-compatible (OpenAI, DeepSeek, Groq, Gemini, OpenRouter, vLLM, etc.) \u2014 content blocks are serialized directly, no conversion needed</li> <li>Anthropic native \u2014 <code>image_url</code> blocks are automatically converted to Anthropic's <code>image/source/base64</code> format</li> </ul> <p>Note: The LLM model itself must support vision. Non-vision models will ignore or fail to interpret image content.</p>"},{"location":"media/#configuration","title":"Configuration","text":"<p>No additional configuration is needed. Vision works automatically when:</p> <ul> <li>The channel is enabled and configured</li> <li>The LLM model supports multimodal/vision input</li> </ul> <p>Optional proxy support for Telegram file downloads:</p> <pre><code>channels:\n  telegram:\n    enabled: true\n    token: \"BOT_TOKEN\"\n    proxy: \"http://proxy.example.com:8080\"  # Optional\n</code></pre>"},{"location":"media/#limits","title":"Limits","text":"<ul> <li>Max image size: 20 MB (configurable via <code>MAX_IMAGE_SIZE</code> constant)</li> <li>Telegram Bot API limit: 20 MB for file downloads</li> <li>Images are not persisted in session history \u2014 only the current turn's images are sent to the LLM to avoid token cost bloat</li> </ul>"},{"location":"media/#architecture-details","title":"Architecture details","text":""},{"location":"media/#mediaattachmentdata","title":"MediaAttachment.data","text":"<p>The <code>data</code> field on <code>MediaAttachment</code> uses <code>@[JSON::Field(ignore: true)]</code> to keep base64 image data out of JSON serialization. This means:</p> <ul> <li>Session files (JSONL) stay small \u2014 no multi-MB base64 strings</li> <li>Past images are not re-sent on subsequent turns</li> <li>The field is only populated for the current inbound message</li> </ul>"},{"location":"media/#content-block-format","title":"Content block format","text":"<p>The context builder produces OpenAI-format content blocks:</p> <pre><code>[\n  {\"type\": \"text\", \"text\": \"Analyze this image\"},\n  {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,...\"}}\n]\n</code></pre> <p>For Anthropic native, this is converted to:</p> <pre><code>[\n  {\"type\": \"text\", \"text\": \"Analyze this image\"},\n  {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": \"...\"}}\n]\n</code></pre>"},{"location":"media/#image-generation","title":"Image generation","text":"<p>The <code>generate_image</code> tool allows the LLM to create images from text prompts and send them directly to users.</p>"},{"location":"media/#how-it-works_1","title":"How it works","text":"<pre><code>User prompt -&gt; LLM -&gt; generate_image tool -&gt; Provider API -&gt; Channel -&gt; User\n</code></pre> <ol> <li>The user asks the LLM to create an image</li> <li>The LLM calls <code>generate_image(prompt)</code> with a description</li> <li>The tool calls the provider's image generation API</li> <li>Base64 image data is wrapped in an <code>OutboundMessage</code> with <code>MediaAttachment</code></li> <li>The channel sends the photo to the user (e.g. Telegram <code>sendPhoto</code>)</li> </ol>"},{"location":"media/#supported-providers_1","title":"Supported providers","text":"Provider Default model API OpenAI <code>gpt-image-1</code> <code>/v1/images/generations</code> Gemini <code>gemini-2.5-flash-image</code> <code>/v1beta/models/{model}:generateContent</code> <p>Note: Anthropic does not support image generation. When no explicit override is set, autobot automatically picks the first available image-capable provider (tries OpenAI, then Gemini). Use <code>tools.image.provider</code> to force a specific one.</p>"},{"location":"media/#configuration_1","title":"Configuration","text":"<p>Image generation is auto-enabled when an OpenAI or Gemini provider is configured. No extra settings needed.</p> <p>To override the provider or model:</p> <pre><code>tools:\n  image:\n    enabled: true\n    provider: openai         # optional, auto-detected from configured providers\n    model: gpt-image-1       # optional, auto-detected from provider\n    size: 1024x1024          # optional, default: 1024x1024\n</code></pre>"},{"location":"media/#supported-channels-outbound","title":"Supported channels (outbound)","text":"Channel Status Notes Telegram Supported Sends photos via <code>sendPhoto</code> multipart API Slack Text fallback Logs warning, sends caption as text Zulip Text fallback Logs warning, sends caption as text"},{"location":"media/#verification","title":"Verification","text":"<p>Run <code>autobot doctor</code> to check image generation status:</p> <pre><code>\u2713 Image generation available (openai)\n</code></pre> <p>Or if no provider is configured:</p> <pre><code>\u2014 Image generation (no openai/gemini provider)\n</code></pre>"},{"location":"media/#voice-transcription","title":"Voice transcription","text":"<p>Voice and audio messages received via Telegram are automatically transcribed to text using the Whisper API before being sent to the LLM.</p>"},{"location":"media/#how-it-works_2","title":"How it works","text":"<pre><code>Telegram voice -&gt; Download OGG -&gt; Transcriber (Whisper API) -&gt; Text in message content -&gt; LLM\n</code></pre> <ol> <li>Channel receives a voice/audio message and downloads the file bytes</li> <li>Transcriber sends the audio to the Whisper API (OpenAI or Groq) and receives text</li> <li>The transcribed text replaces the <code>[voice message]</code> placeholder as <code>[voice transcription]: {text}</code></li> <li>The LLM receives the transcription as regular text content</li> </ol>"},{"location":"media/#configuration_2","title":"Configuration","text":"<p>No extra configuration needed. Voice transcription is auto-enabled when a Whisper-capable provider (Groq or OpenAI) is configured:</p> <pre><code>providers:\n  groq:\n    api_key: \"${GROQ_API_KEY}\"  # Voice transcription auto-enabled via Groq Whisper\n</code></pre> <p>Or:</p> <pre><code>providers:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"  # Voice transcription auto-enabled via OpenAI Whisper\n</code></pre> <p>Groq is preferred when both are configured (faster, has free tier). If neither is configured, voice messages fall back to <code>[voice message]</code> text with no errors.</p>"},{"location":"media/#supported-providers_2","title":"Supported providers","text":"Provider Model API endpoint Groq <code>whisper-large-v3-turbo</code> <code>api.groq.com/openai/v1/audio/transcriptions</code> OpenAI <code>whisper-1</code> <code>api.openai.com/v1/audio/transcriptions</code>"},{"location":"media/#verification_1","title":"Verification","text":"<p>Run <code>autobot doctor</code> to check voice transcription status:</p> <pre><code>\u2713 Voice transcription available (groq)\n</code></pre> <p>Or if no provider is configured:</p> <pre><code>\u2014 Voice transcription (no openai/groq provider)\n</code></pre>"},{"location":"memory/","title":"Memory Management","text":"<p>Autobot implements a two-layer memory system with automatic consolidation to manage long conversations efficiently.</p>"},{"location":"memory/#overview","title":"Overview","text":"<p>As conversations grow, sending all previous messages to the LLM becomes:</p> <ul> <li>Expensive - More tokens = higher costs</li> <li>Slow - Larger context = slower responses</li> <li>Limited - Eventually hits model context limits (200K tokens for Claude)</li> </ul> <p>Memory consolidation solves this by:</p> <ol> <li>Summarizing old messages into compact memory</li> <li>Keeping only recent messages in full context</li> <li>Archiving important facts for later retrieval</li> </ol>"},{"location":"memory/#two-layer-memory","title":"Two-Layer Memory","text":""},{"location":"memory/#1-memorymd-long-term-facts","title":"1. MEMORY.md (Long-term Facts)","text":"<p>Location: <code>workspace/memory/MEMORY.md</code></p> <p>Stores important facts that should persist across sessions:</p> <ul> <li>User information (location, preferences, habits)</li> <li>Project context (tech stack, architecture decisions)</li> <li>Important decisions and outcomes</li> <li>Tools and services used</li> </ul> <p>Updated: During memory consolidation when new facts are learned.</p> <p>Usage: Loaded into every LLM request as part of system context.</p>"},{"location":"memory/#2-historymd-searchable-log","title":"2. HISTORY.md (Searchable Log)","text":"<p>Location: <code>workspace/memory/HISTORY.md</code></p> <p>Append-only log of consolidated conversation summaries:</p> <ul> <li>Each entry is 2-5 sentences with timestamp</li> <li>Grep-searchable for finding past discussions</li> <li>Contains enough detail to recall context</li> </ul> <p>Updated: After each consolidation with new summary entry.</p> <p>Usage: Search with <code>grep \"keyword\" workspace/memory/HISTORY.md</code> to recall past context.</p>"},{"location":"memory/#how-consolidation-works","title":"How Consolidation Works","text":""},{"location":"memory/#trigger","title":"Trigger","text":"<p>Consolidation runs automatically when: <pre><code>session.messages.size &gt; memory_window\n</code></pre></p> <p>Default <code>memory_window: 50</code> means consolidation after 50 messages.</p>"},{"location":"memory/#process","title":"Process","text":"<ol> <li>Extract old messages</li> <li>Keep last 10 messages (recent context)</li> <li> <p>Archive everything older for consolidation</p> </li> <li> <p>Send to LLM (parallel, non-blocking)</p> </li> <li>Format old messages as conversation history</li> <li>Include current MEMORY.md content</li> <li> <p>Ask LLM to:</p> <ul> <li>Write 2-5 sentence summary for HISTORY.md</li> <li>Update MEMORY.md with any new facts</li> </ul> </li> <li> <p>Update files</p> </li> <li>Append summary to HISTORY.md</li> <li> <p>Replace MEMORY.md if facts changed</p> </li> <li> <p>Trim session</p> </li> <li>Keep only last 10 messages</li> <li>Save trimmed session to disk</li> </ol>"},{"location":"memory/#parallel-execution","title":"Parallel Execution","text":"<p>Consolidation runs in a background fiber: <pre><code>spawn do\n  perform_consolidation(...)\nend\n</code></pre></p> <p>Benefits:</p> <ul> <li>Agent continues processing immediately</li> <li>No blocking wait for LLM response</li> <li>User sees faster response times</li> </ul>"},{"location":"memory/#configuration","title":"Configuration","text":""},{"location":"memory/#enable-consolidation-default","title":"Enable Consolidation (Default)","text":"<pre><code>agents:\n  defaults:\n    memory_window: 50  # Consolidate after 50 messages\n</code></pre> <p>When to use:</p> <ul> <li>Long-running conversations</li> <li>Want searchable history</li> <li>Need persistent facts across sessions</li> </ul> <p>Behavior:</p> <ul> <li>Consolidates after N messages</li> <li>Keeps last 10 messages in full context</li> <li>Archives older messages as summaries</li> </ul>"},{"location":"memory/#disable-consolidation","title":"Disable Consolidation","text":"<pre><code>agents:\n  defaults:\n    memory_window: 0  # Disable consolidation\n</code></pre> <p>When to use:</p> <ul> <li>Short conversations (under 50 messages)</li> <li>Testing/development</li> <li>Want full message history always available</li> <li>Don't want LLM-generated summaries</li> </ul> <p>Behavior:</p> <ul> <li>No consolidation happens</li> <li>Keeps only last 10 messages (simple trim)</li> <li>No MEMORY.md or HISTORY.md updates</li> <li>Eventually hits context limits on very long conversations</li> </ul>"},{"location":"memory/#custom-window-size","title":"Custom Window Size","text":"<pre><code>agents:\n  defaults:\n    memory_window: 100  # Consolidate after 100 messages\n</code></pre> <p>Lower values (20-40):</p> <ul> <li>More frequent consolidation</li> <li>Smaller context windows</li> <li>Lower costs per request</li> <li>More aggressive summarization</li> </ul> <p>Higher values (80-150):</p> <ul> <li>Less frequent consolidation</li> <li>Larger context windows</li> <li>More detailed recent history</li> <li>Higher costs per request</li> </ul>"},{"location":"memory/#constants","title":"Constants","text":"<p>Memory behavior is controlled by these constants in <code>MemoryManager</code>:</p> Constant Value Description <code>DISABLED_MEMORY_WINDOW</code> 0 Setting <code>memory_window: 0</code> disables consolidation <code>MIN_KEEP_COUNT</code> 2 Minimum messages to keep after consolidation <code>MAX_KEEP_COUNT</code> 10 Maximum messages to keep after consolidation <code>MAX_MESSAGES_WITHOUT_CONSOLIDATION</code> 10 When disabled, trim to this many messages"},{"location":"memory/#file-structure","title":"File Structure","text":"<pre><code>workspace/\n\u251c\u2500\u2500 memory/\n\u2502   \u251c\u2500\u2500 MEMORY.md     # Long-term facts (replaced on update)\n\u2502   \u2514\u2500\u2500 HISTORY.md    # Searchable log (append-only)\n\u2514\u2500\u2500 sessions/\n    \u2514\u2500\u2500 session.jsonl # Full message history\n</code></pre> <p>Permissions:</p> <ul> <li><code>memory/</code> directory: <code>0o700</code> (user-only access)</li> <li><code>MEMORY.md</code>: <code>0o600</code> (user read/write only)</li> <li><code>HISTORY.md</code>: <code>0o600</code> (user read/write only)</li> </ul> <p>Set automatically by <code>autobot new</code> command.</p>"},{"location":"memory/#examples","title":"Examples","text":""},{"location":"memory/#example-1-enabled-with-default-window","title":"Example 1: Enabled with Default Window","text":"<p>Config: <pre><code>agents:\n  defaults:\n    memory_window: 50\n</code></pre></p> <p>Behavior: <pre><code>Messages 1-40:  Normal conversation\nMessage 51:     Consolidation triggers\n                - Keep messages 41-51 in context\n                - Summarize messages 1-40\n                - Update MEMORY.md + HISTORY.md\nMessage 52-101: Normal conversation\nMessage 102:    Consolidation triggers again\n                - Keep messages 92-102\n                - Summarize messages 41-91\n</code></pre></p>"},{"location":"memory/#example-2-disabled","title":"Example 2: Disabled","text":"<p>Config: <pre><code>agents:\n  defaults:\n    memory_window: 0\n</code></pre></p> <p>Behavior: <pre><code>Messages 1-10:  Normal conversation\nMessage 11:     Trim to last 10 messages (keep 2-11)\nMessage 12:     Trim to last 10 messages (keep 3-12)\n...\nNo consolidation, no MEMORY.md updates, no HISTORY.md entries\n</code></pre></p>"},{"location":"memory/#example-3-aggressive-consolidation","title":"Example 3: Aggressive Consolidation","text":"<p>Config: <pre><code>agents:\n  defaults:\n    memory_window: 20\n</code></pre></p> <p>Behavior: <pre><code>Message 21:  Consolidate (keep 11-21)\nMessage 41:  Consolidate (keep 31-41)\nMessage 61:  Consolidate (keep 51-61)\n...\nFrequent consolidation, smaller context, lower cost per request\n</code></pre></p>"},{"location":"memory/#searching-memory","title":"Searching Memory","text":""},{"location":"memory/#search-historymd","title":"Search HISTORY.md","text":"<p>Find past discussions about specific topics: <pre><code># Search for all mentions of \"deployment\"\ngrep -i \"deployment\" workspace/memory/HISTORY.md\n\n# Search with context (2 lines before/after)\ngrep -C2 \"api changes\" workspace/memory/HISTORY.md\n\n# Search by date\ngrep \"2025-01-15\" workspace/memory/HISTORY.md\n</code></pre></p>"},{"location":"memory/#read-memorymd","title":"Read MEMORY.md","text":"<p>View current long-term facts: <pre><code>cat workspace/memory/MEMORY.md\n</code></pre></p> <p>The LLM automatically reads this file at the start of each conversation.</p>"},{"location":"memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"memory/#consolidation-not-happening","title":"Consolidation not happening","text":"<p>Check:</p> <ol> <li>Is <code>memory_window</code> set to 0? (disabled)</li> <li>Have you exceeded the window size?</li> <li>Check logs for consolidation errors</li> </ol> <p>Solution: <pre><code>agents:\n  defaults:\n    memory_window: 50  # Ensure it's not 0\n</code></pre></p>"},{"location":"memory/#consolidation-too-slow","title":"Consolidation too slow","text":"<p>Symptoms: Long pauses during conversation</p> <p>Cause: Consolidation is CPU/network intensive</p> <p>Solution: Consolidation already runs in parallel (non-blocking). If still slow:</p> <ul> <li>Increase <code>memory_window</code> to consolidate less often</li> <li>Use faster model for consolidation</li> <li>Check network latency to LLM provider</li> </ul>"},{"location":"memory/#memory-files-have-wrong-permissions","title":"Memory files have wrong permissions","text":"<p>Symptoms: Permission denied errors</p> <p>Cause: Files created by different user (e.g., root in Docker)</p> <p>Solution: <pre><code># Fix ownership (replace 1000:1000 with your user:group)\nchown -R 1000:1000 workspace/memory/\n\n# Or fix permissions\nchmod 700 workspace/memory/\nchmod 600 workspace/memory/*.md\n</code></pre></p>"},{"location":"memory/#context-limit-still-exceeded","title":"Context limit still exceeded","text":"<p>Symptoms: Error about context window even with consolidation enabled</p> <p>Cause: <code>memory_window</code> set too high, or individual messages are very large</p> <p>Solution:</p> <ul> <li>Lower <code>memory_window</code> to 30-40</li> <li>Break up very long messages into smaller chunks</li> <li>Consider disabling and manually managing conversation length</li> </ul>"},{"location":"memory/#best-practices","title":"Best Practices","text":""},{"location":"memory/#for-long-conversations","title":"For Long Conversations","text":"<ul> <li>Enable consolidation (<code>memory_window: 50</code>)</li> <li>Periodically review <code>MEMORY.md</code> for accuracy</li> <li>Use <code>grep</code> on <code>HISTORY.md</code> to recall past topics</li> </ul>"},{"location":"memory/#for-short-sessions","title":"For Short Sessions","text":"<ul> <li>Disable consolidation (<code>memory_window: 0</code>)</li> <li>Simpler, faster, no LLM overhead</li> <li>Suitable for quick tasks under 50 messages</li> </ul>"},{"location":"memory/#for-cost-optimization","title":"For Cost Optimization","text":"<ul> <li>Lower window (<code>memory_window: 30</code>)</li> <li>More aggressive consolidation</li> <li>Smaller context = lower token costs</li> <li>Trade-off: less recent context available</li> </ul>"},{"location":"memory/#for-maximum-context","title":"For Maximum Context","text":"<ul> <li>Higher window (<code>memory_window: 100</code>)</li> <li>Less frequent consolidation</li> <li>More full messages in context</li> <li>Trade-off: higher costs per request</li> </ul>"},{"location":"memory/#technical-details","title":"Technical Details","text":""},{"location":"memory/#consolidation-prompt","title":"Consolidation Prompt","text":"<p>The LLM receives: <pre><code>Current Long-term Memory: &lt;content of MEMORY.md&gt;\n\nConversation to Process: &lt;formatted old messages&gt;\n\nReturn JSON:\n{\n  \"history_entry\": \"2025-01-15 10:30: User asked about deployment. Discussed Docker setup and CI/CD pipeline. Decided on GitHub Actions.\",\n  \"memory_update\": \"&lt;updated MEMORY.md with new facts&gt;\"\n}\n</code></pre></p>"},{"location":"memory/#message-format","title":"Message Format","text":"<p>Messages sent for consolidation: <pre><code>[2025-01-15 10:30] USER: how do I deploy this?\n[2025-01-15 10:31] ASSISTANT [tools: exec, read_file]: I'll help you...\n[2025-01-15 10:32] USER: what about Docker?\n</code></pre></p>"},{"location":"memory/#session-trimming","title":"Session Trimming","text":"<p>After consolidation: <pre><code>session.messages = session.messages[-keep_count..]  # Keep last 10\n@sessions.save(session)\n</code></pre></p> <p>Only recent messages remain in the active session file.</p>"},{"location":"memory/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Full config options</li> <li>Security Guide - File permissions and isolation</li> <li>Architecture Overview - System design</li> </ul>"},{"location":"openai/","title":"OpenAI","text":"<p>Autobot supports OpenAI as an LLM provider via the Chat Completions API. This gives access to the GPT-5 family, o3, and other OpenAI models. The mini and nano variants offer a great balance of capability and cost for everyday tasks.</p>"},{"location":"openai/#setup","title":"Setup","text":""},{"location":"openai/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at platform.openai.com/api-keys.</p>"},{"location":"openai/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=sk-...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"OpenAI (GPT)\" as provider\n</code></pre>"},{"location":"openai/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"openai/gpt-5-mini\"\n\nproviders:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n</code></pre>"},{"location":"openai/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (openai)\n</code></pre>"},{"location":"openai/#model-naming","title":"Model naming","text":"<p>Models use the <code>openai/</code> prefix followed by the OpenAI model ID:</p> <pre><code># GPT-5 family (recommended)\nmodel: \"openai/gpt-5-mini\"       # Fast, cheap, capable (recommended default)\nmodel: \"openai/gpt-5-nano\"       # Fastest, cheapest \u2014 great for simple tasks\nmodel: \"openai/gpt-5.2\"          # Flagship \u2014 smartest, most precise\n\n# Reasoning\nmodel: \"openai/o3\"               # Complex reasoning tasks\n\n# Coding\nmodel: \"openai/gpt-5.3-codex\"   # Best for agentic coding\n\n# Legacy (still available in the API)\nmodel: \"openai/gpt-4.1\"\nmodel: \"openai/gpt-4.1-mini\"\nmodel: \"openai/gpt-4.1-nano\"\n</code></pre> <p><code>gpt-5-mini</code> is a good default for most use cases \u2014 significantly cheaper and faster than the flagship while still handling tool use, coding, and reasoning well. Use <code>gpt-5-nano</code> for high-volume, simpler tasks like summarization or classification.</p> <p>The <code>openai/</code> prefix tells autobot to route to the OpenAI API. It is stripped before sending to the API.</p> <p>See the full model list in the OpenAI docs.</p>"},{"location":"openai/#custom-api-base","title":"Custom API base","text":"<p>To use a self-hosted proxy, Azure OpenAI, or any OpenAI-compatible endpoint:</p> <pre><code>providers:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    api_base: \"https://your-proxy.example.com/v1/chat/completions\"\n</code></pre>"},{"location":"openai/#extra-headers","title":"Extra headers","text":"<p>Pass additional headers with every request:</p> <pre><code>providers:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    extra_headers:\n      X-Custom-Header: \"value\"\n</code></pre>"},{"location":"openai/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 OpenAI API key (<code>sk-...</code>) <code>api_base</code> No <code>https://api.openai.com/v1/chat/completions</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"openai/#how-it-works","title":"How it works","text":"<p>OpenAI uses the standard Chat Completions API format, which is the baseline for most LLM providers:</p> <ul> <li><code>Authorization: Bearer</code> header for authentication</li> <li>Standard message format with <code>role</code> and <code>content</code> fields</li> <li>Function calling via <code>tools</code> array with <code>tool_choice</code></li> </ul> <p>Autobot detects OpenAI models by keywords (<code>openai</code>, <code>gpt</code>, <code>o1</code>, <code>o3</code>, <code>o4</code>) and routes to the correct endpoint automatically. All GPT-5 family models, o3, and legacy GPT-4.1 models are supported. Tools, MCP servers, plugins, and all other features work seamlessly.</p>"},{"location":"openai/#voice-transcription","title":"Voice transcription","text":"<p>OpenAI provides the Whisper API for voice transcription. When OpenAI is configured, voice messages are automatically transcribed using <code>whisper-1</code>. No extra configuration is needed \u2014 the API key is reused from the provider config.</p> <p>If both Groq and OpenAI are configured, Groq is preferred for transcription (faster, free tier available).</p>"},{"location":"openai/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> </ul>"},{"location":"openai/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://api.openai.com/v1/chat/completions model=...</code> \u2014 confirms provider is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"openai/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: Incorrect API key provided\" \u2014 Invalid or revoked API key. Verify at platform.openai.com/api-keys.</p> <p>\"API error: Rate limit reached\" \u2014 Too many requests or tokens. Check your usage at platform.openai.com/usage.</p> <p>\"API error: The model does not exist\" \u2014 Model ID is wrong, or you don't have access to the model. Check available models in your account.</p>"},{"location":"openrouter/","title":"OpenRouter","text":"<p>Autobot supports OpenRouter as a gateway provider. OpenRouter aggregates hundreds of models from multiple providers (Anthropic, OpenAI, Google, Meta, Mistral, and more) behind a single API key and unified billing.</p>"},{"location":"openrouter/#setup","title":"Setup","text":""},{"location":"openrouter/#1-get-an-api-key","title":"1. Get an API key","text":"<p>Create an API key at openrouter.ai/keys.</p>"},{"location":"openrouter/#2-configure-credentials","title":"2. Configure credentials","text":"<p>Add your API key to the <code>.env</code> file:</p> <pre><code>OPENROUTER_API_KEY=sk-or-...\n</code></pre> <p>Or use the interactive setup:</p> <pre><code>autobot setup\n# Select \"OpenRouter\" as provider\n</code></pre>"},{"location":"openrouter/#3-configure-the-provider","title":"3. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"openrouter/anthropic/claude-sonnet-4-5\"\n\nproviders:\n  openrouter:\n    api_key: \"${OPENROUTER_API_KEY}\"\n</code></pre>"},{"location":"openrouter/#4-verify","title":"4. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (openrouter)\n</code></pre>"},{"location":"openrouter/#model-naming","title":"Model naming","text":"<p>Models use the <code>openrouter/</code> prefix followed by the OpenRouter model path:</p> <pre><code># Anthropic via OpenRouter\nmodel: \"openrouter/anthropic/claude-sonnet-4-5\"\nmodel: \"openrouter/anthropic/claude-3.5-haiku\"\n\n# OpenAI via OpenRouter\nmodel: \"openrouter/openai/gpt-4o\"\nmodel: \"openrouter/openai/o3-mini\"\n\n# Google via OpenRouter\nmodel: \"openrouter/google/gemini-2.5-pro\"\n\n# Meta via OpenRouter\nmodel: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n\n# Auto-routing (picks the best model automatically)\nmodel: \"openrouter/auto\"\n</code></pre> <p>The <code>openrouter/</code> prefix tells autobot to route through OpenRouter's API. OpenRouter model IDs include the original provider as part of the path (e.g. <code>anthropic/claude-sonnet-4-5</code>).</p> <p>See the full model list at openrouter.ai/models.</p>"},{"location":"openrouter/#auto-detection","title":"Auto-detection","text":"<p>OpenRouter is auto-detected in two ways:</p> <ul> <li>By API key prefix \u2014 Keys starting with <code>sk-or-</code> are recognized as OpenRouter</li> <li>By API base URL \u2014 If <code>api_base</code> contains <code>openrouter</code>, the gateway is detected automatically</li> </ul> <p>This means you can also use OpenRouter without explicit <code>openrouter/</code> model prefixes if the API key or base URL identifies it.</p>"},{"location":"openrouter/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes \u2014 OpenRouter API key (<code>sk-or-...</code>) <code>api_base</code> No <code>https://openrouter.ai/api/v1/chat/completions</code> Custom API endpoint <code>extra_headers</code> No \u2014 Additional HTTP headers for every request"},{"location":"openrouter/#how-it-works","title":"How it works","text":"<p>OpenRouter acts as a gateway \u2014 it accepts OpenAI-compatible requests and forwards them to the upstream provider:</p> <ul> <li><code>Authorization: Bearer</code> header for authentication</li> <li>Standard OpenAI-compatible format for all requests</li> <li>Model prefix \u2014 autobot automatically prepends <code>openrouter/</code> to model names for routing</li> </ul> <p>Since OpenRouter forwards to the original provider, the underlying model's capabilities apply (tool use, context window, etc.). Autobot always uses the OpenAI-compatible path through OpenRouter, even for Anthropic models.</p>"},{"location":"openrouter/#voice-transcription","title":"Voice transcription","text":"<p>OpenRouter does not provide a transcription API. If you need voice message support, configure an additional Groq or OpenAI provider for Whisper-based transcription.</p>"},{"location":"openrouter/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>No native Anthropic path \u2014 Even when using Claude models through OpenRouter, autobot uses the OpenAI-compatible format (OpenRouter handles the conversion).</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> <li>Per-model limits vary \u2014 Rate limits and pricing depend on the underlying model. Check openrouter.ai/models for details.</li> </ul>"},{"location":"openrouter/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST https://openrouter.ai/api/v1/chat/completions model=...</code> \u2014 confirms gateway is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms API response</li> <li><code>HTTP 4xx/5xx: ...</code> \u2014 API errors with details</li> </ul>"},{"location":"openrouter/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set and non-empty in <code>config.yml</code>.</p> <p>\"API error: invalid_api_key\" \u2014 Invalid or revoked API key. Verify at openrouter.ai/keys.</p> <p>\"API error: insufficient_quota\" \u2014 Out of credits. Top up at openrouter.ai/credits.</p> <p>\"API error: model_not_found\" \u2014 Model path is wrong or has been removed. Check the OpenRouter models page for current availability.</p>"},{"location":"plugins/","title":"Plugins","text":"<p>Autobot plugins are Crystal classes that extend runtime behavior by registering tools and lifecycle hooks.</p>"},{"location":"plugins/#plugin-lifecycle","title":"Plugin Lifecycle","text":"<p>Each plugin can implement:</p> <ul> <li><code>setup(context)</code></li> <li><code>start</code></li> <li><code>stop</code></li> </ul> <p>During setup, register tools through <code>context.tool_registry</code>.</p>"},{"location":"plugins/#minimal-plugin","title":"Minimal Plugin","text":"<pre><code>require \"autobot\"\n\nmodule MyPlugin\n  class Echo &lt; Autobot::Plugins::Plugin\n    def name : String\n      \"echo\"\n    end\n\n    def description : String\n      \"Simple echo plugin\"\n    end\n\n    def version : String\n      \"0.1.0\"\n    end\n\n    def setup(context : Autobot::Plugins::PluginContext) : Nil\n      context.tool_registry.register(EchoTool.new)\n    end\n  end\n\n  class EchoTool &lt; Autobot::Tools::Tool\n    def name : String\n      \"echo\"\n    end\n\n    def description : String\n      \"Echo input text\"\n    end\n\n    def parameters : Autobot::Tools::ToolSchema\n      Autobot::Tools::ToolSchema.new(\n        properties: {\n          \"text\" =&gt; Autobot::Tools::PropertySchema.new(\n            type: \"string\",\n            description: \"Text to echo\"\n          ),\n        },\n        required: [\"text\"]\n      )\n    end\n\n    def execute(params : Hash(String, JSON::Any)) : String\n      params[\"text\"].as_s\n    end\n  end\nend\n\nAutobot::Plugins::Loader.register(MyPlugin::Echo.new)\n</code></pre>"},{"location":"plugins/#add-plugin-to-your-app","title":"Add Plugin to Your App","text":"<pre><code>require \"autobot\"\nrequire \"my-plugin\"\n\nAutobot.run\n</code></pre>"},{"location":"plugins/#notes","title":"Notes","text":"<ul> <li>Plugin names should be unique.</li> <li><code>Loader.register(...)</code> queues plugins before startup.</li> <li><code>autobot status</code> shows loaded plugin metadata.</li> </ul>"},{"location":"providers/","title":"Providers","text":"<p>Autobot supports multiple LLM providers out of the box. Configure at least one provider to get started.</p>"},{"location":"providers/#supported-providers","title":"Supported providers","text":"Provider Type Voice transcription Notes Anthropic Direct \u2014 Claude models via native Messages API OpenAI Direct Whisper GPT-5 family, o3 models DeepSeek Direct \u2014 DeepSeek-V3, R1 with reasoning traces Groq Direct Whisper (preferred) Ultra-fast inference on LPU hardware Google Gemini Direct \u2014 Gemini Pro, Flash models OpenRouter Gateway \u2014 Hundreds of models, single API key AWS Bedrock Cloud \u2014 Claude, Nova via AWS SigV4 auth vLLM / Local Local \u2014 Self-hosted, any OpenAI-compatible server <p>Direct providers connect to the provider's own API. Gateway providers aggregate multiple upstream providers behind a single key. Local providers run on your own hardware.</p>"},{"location":"providers/#quick-comparison","title":"Quick comparison","text":"<p>Best for getting started \u2014 Anthropic or OpenAI. Widely used, well-documented APIs.</p> <p>Best for speed \u2014 Groq. Extremely fast inference with generous free tier.</p> <p>Best for cost \u2014 DeepSeek. Strong models at low per-token pricing.</p> <p>Best for variety \u2014 OpenRouter. Access hundreds of models with one API key.</p> <p>Best for enterprise \u2014 AWS Bedrock. Runs in your AWS account with IAM-based access control.</p> <p>Best for privacy \u2014 vLLM / Local. Data never leaves your machine.</p>"},{"location":"providers/#model-naming-convention","title":"Model naming convention","text":"<p>All models use a <code>provider/model-id</code> format:</p> <pre><code>model: \"anthropic/claude-sonnet-4-5\"\nmodel: \"openai/gpt-5-mini\"\nmodel: \"deepseek/deepseek-chat\"\nmodel: \"groq/llama-3.3-70b-versatile\"\nmodel: \"gemini/gemini-2.5-flash\"\nmodel: \"openrouter/anthropic/claude-sonnet-4-5\"\nmodel: \"bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\nmodel: \"vllm/meta-llama/Llama-3.3-70B-Instruct\"\n</code></pre> <p>The prefix tells autobot which provider to use. It is stripped before sending to the API (except for gateway providers like OpenRouter, where the model path is forwarded).</p>"},{"location":"providers/#voice-transcription","title":"Voice transcription","text":"<p>Voice messages are automatically transcribed when a supported provider is configured:</p> <ul> <li>Groq (preferred) \u2014 uses <code>whisper-large-v3-turbo</code>, faster with free tier</li> <li>OpenAI \u2014 uses <code>whisper-1</code></li> </ul> <p>If neither is configured, voice messages fall back to <code>[voice message]</code> text. Transcription works regardless of which provider you use for chat \u2014 you can use DeepSeek for chat and Groq for voice transcription by configuring both.</p>"},{"location":"providers/#multiple-providers","title":"Multiple providers","text":"<p>You can configure multiple providers simultaneously. Autobot selects the provider based on the model prefix in your config:</p> <pre><code>providers:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"\n  groq:\n    api_key: \"${GROQ_API_KEY}\"\n\nagents:\n  defaults:\n    model: \"anthropic/claude-sonnet-4-5\"  # Uses Anthropic for chat\n                                           # Uses Groq for voice transcription\n</code></pre>"},{"location":"providers/#feature-compatibility","title":"Feature compatibility","text":"<p>All providers support the same autobot features:</p> <ul> <li>Tool use and function calling</li> <li>MCP servers</li> <li>Plugins</li> <li>Memory system</li> <li>Cron scheduling</li> <li>All chat channels (Telegram, Slack, WhatsApp, Zulip, CLI)</li> </ul> <p>The only exception is voice transcription, which requires Groq or OpenAI (see above).</p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#1-install","title":"1. Install","text":""},{"location":"quickstart/#option-a-homebrew-macos","title":"Option A: Homebrew (macOS)","text":"<pre><code>brew tap crystal-autobot/tap\nbrew install autobot\n</code></pre>"},{"location":"quickstart/#option-b-download-binary","title":"Option B: Download binary","text":"<pre><code># Automatic detection (Linux/macOS)\ncurl -L \"https://github.com/crystal-autobot/autobot/releases/latest/download/autobot-$(uname -s | tr '[:upper:]' '[:lower:]')-$(uname -m)\" -o autobot\nchmod +x autobot\nsudo mv autobot /usr/local/bin/\n\n# Or manually download from releases page:\n# https://github.com/crystal-autobot/autobot/releases\n</code></pre>"},{"location":"quickstart/#option-c-build-from-source","title":"Option C: Build from source","text":"<pre><code>git clone https://github.com/crystal-autobot/autobot.git\ncd autobot\nmake release\nsudo install -m 0755 bin/autobot /usr/local/bin/autobot\n</code></pre>"},{"location":"quickstart/#option-d-docker","title":"Option D: Docker","text":"<pre><code>docker pull ghcr.io/crystal-autobot/autobot:latest\ndocker run --rm -it \\\n  -v ./my-bot:/app \\\n  -e ANTHROPIC_API_KEY=sk-ant-... \\\n  ghcr.io/crystal-autobot/autobot:latest gateway\n</code></pre>"},{"location":"quickstart/#2-create-a-new-bot","title":"2. Create a new bot","text":"<pre><code>autobot new optimus\ncd optimus\n</code></pre> <p>This creates an <code>optimus/</code> directory with the following structure:</p> <pre><code>optimus/\n\u251c\u2500\u2500 .env              # API keys (0600 permissions)\n\u251c\u2500\u2500 .gitignore        # Excludes secrets, sessions, logs, memory\n\u251c\u2500\u2500 config.yml        # Configuration (references .env via ${ENV_VAR})\n\u251c\u2500\u2500 sessions/         # Conversation history (JSONL)\n\u251c\u2500\u2500 logs/             # Application logs\n\u2514\u2500\u2500 workspace/        # Sandboxed LLM workspace (0700 permissions)\n    \u251c\u2500\u2500 AGENTS.md     # Agent instructions\n    \u251c\u2500\u2500 SOUL.md       # Personality definition\n    \u251c\u2500\u2500 USER.md       # User preferences\n    \u251c\u2500\u2500 memory/       # Long-term memory\n    \u2502   \u251c\u2500\u2500 MEMORY.md\n    \u2502   \u2514\u2500\u2500 HISTORY.md\n    \u2514\u2500\u2500 skills/       # Custom skills\n</code></pre> <p>The name is arbitrary \u2014 use whatever you like (<code>autobot new my-bot</code>, <code>autobot new work-assistant</code>, etc.).</p>"},{"location":"quickstart/#3-configure","title":"3. Configure","text":"<p>Edit <code>.env</code> and add at least one API key:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>The generated <code>config.yml</code> references these via <code>${ENV_VAR}</code> syntax \u2014 secrets stay in <code>.env</code>, never in config files.</p> <p>To enable channels, uncomment and fill in the relevant tokens in <code>.env</code>, then update <code>config.yml</code>:</p> <pre><code>channels:\n  telegram:\n    enabled: true\n    token: \"${TELEGRAM_BOT_TOKEN}\"\n    allow_from: [\"your_user_id\"]\n</code></pre>"},{"location":"quickstart/#4-validate","title":"4. Validate","text":"<pre><code>autobot doctor\n</code></pre> <p>Checks configuration, security settings, sandbox availability, and file permissions. Use <code>--strict</code> to treat warnings as errors.</p>"},{"location":"quickstart/#5-run","title":"5. Run","text":"<pre><code># Interactive terminal mode\nautobot agent\n\n# Single message\nautobot agent -m \"Hello!\"\n\n# Gateway mode (all enabled channels)\nautobot gateway\n</code></pre>"},{"location":"quickstart/#next","title":"Next","text":"<ul> <li>Configuration</li> <li>Security</li> <li>Deployment</li> </ul>"},{"location":"sandboxing/","title":"Sandboxing Architecture","text":"<p>Autobot uses kernel-level sandboxing to safely restrict LLM file access. Each operation spawns a sandboxed process via bubblewrap or Docker.</p>"},{"location":"sandboxing/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sandbox.exec                                          \u2502\n\u2502  \u2022 Works everywhere, zero setup                       \u2502\n\u2502  \u2022 Single binary                                      \u2502\n\u2502  \u2022 Spawns sandbox per operation                       \u2502\n\u2502  \u2022 Uses shell commands (cat, ls, base64)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sandboxing/#how-it-works","title":"How It Works","text":"<p>Instead of spawning a persistent server, we spawn a sandboxed process for each operation:</p> <pre><code># Read file\nSandbox.exec(\"cat #{shell_escape(path)} 2&gt;&amp;1\", workspace, timeout: 10)\n\n# Write file (using base64 to avoid escaping issues)\nencoded = Base64.strict_encode(content)\nSandbox.exec(\"printf '%s' '#{encoded}' | base64 -d &gt; #{shell_escape(path)}\", workspace, timeout: 30)\n\n# List directory\nSandbox.exec(\"ls -1a #{shell_escape(path)} 2&gt;&amp;1\", workspace, timeout: 10)\n</code></pre>"},{"location":"sandboxing/#why-shell-commands","title":"Why Shell Commands?","text":"<ul> <li>Alpine container has <code>/bin/sh</code> built-in - no binary compatibility issues</li> <li>We pass strings, not binaries - works everywhere</li> <li>Works in Docker/bubblewrap/any Linux container</li> <li>Simple and reliable</li> </ul>"},{"location":"sandboxing/#execution-linux-bubblewrap","title":"Execution (Linux - bubblewrap)","text":"<pre><code>bwrap \\\n  --ro-bind /usr /usr \\\n  --ro-bind /bin /bin \\\n  --bind /workspace /workspace \\\n  --unshare-all \\\n  --proc /proc \\\n  --dev /dev \\\n  --chdir /workspace \\\n  -- sh -c \"cat file.txt\"\n</code></pre>"},{"location":"sandboxing/#execution-macosuniversal-docker","title":"Execution (macOS/Universal - Docker)","text":"<pre><code>docker run --rm \\\n  -v /workspace:/workspace:rw \\\n  -w /workspace \\\n  --memory 512m --cpus 1 \\\n  alpine:latest \\\n  sh -c \"cat file.txt\"\n</code></pre>"},{"location":"sandboxing/#platform-support","title":"Platform Support","text":"Platform Sandbox Tool Linux bubblewrap (recommended) Linux Docker macOS Docker Windows Docker (WSL2)"},{"location":"sandboxing/#installation","title":"Installation","text":""},{"location":"sandboxing/#linux-recommended-bubblewrap","title":"Linux (Recommended: bubblewrap)","text":"<pre><code># Ubuntu/Debian\nsudo apt install bubblewrap\n\n# Fedora\nsudo dnf install bubblewrap\n\n# Arch\nsudo pacman -S bubblewrap\n</code></pre>"},{"location":"sandboxing/#macos-requires-docker","title":"macOS (Requires Docker)","text":"<pre><code># Docker Desktop required\n# Download from: https://docs.docker.com/desktop/install/mac-install/\n\n# Verify\ndocker run --rm alpine:latest echo \"Sandbox ready\"\n</code></pre> <p>Why Docker on macOS?</p> <ul> <li>macOS sandbox-exec only restricts writes, NOT reads</li> <li>Can't prevent reading <code>/etc/passwd</code>, <code>~/.ssh/</code>, etc.</li> <li>Docker provides full read+write isolation</li> <li>Apple is deprecating sandbox-exec anyway</li> </ul>"},{"location":"sandboxing/#windows-docker-via-wsl2","title":"Windows (Docker via WSL2)","text":"<pre><code># Install Docker Desktop with WSL2 backend\n# https://docs.docker.com/desktop/windows/wsl/\n\n# Verify\ndocker run --rm alpine:latest echo \"Sandbox ready\"\n</code></pre>"},{"location":"sandboxing/#configuration","title":"Configuration","text":"<p>Configure sandboxing in <code>config.yml</code>:</p> <pre><code>tools:\n  sandbox: auto  # auto | bubblewrap | docker | none (default: auto)\n  docker_image: \"python:3.14-alpine\"  # optional, default: alpine:latest\n</code></pre> <p>Options:</p> <ul> <li><code>sandbox</code> \u2014 Sandbox backend</li> <li><code>auto</code> - Auto-detect best available (recommended)</li> <li><code>bubblewrap</code> - Force bubblewrap (Linux only)</li> <li><code>docker</code> - Force Docker (all platforms)</li> <li><code>none</code> - Disable sandboxing (UNSAFE - tests only)</li> <li><code>docker_image</code> \u2014 Docker image to use for sandbox containers (default: <code>alpine:latest</code>). Set this when your commands need runtimes not available in Alpine (e.g. Python, Node.js). Only applies when sandbox is <code>docker</code> or <code>auto</code> resolves to Docker.</li> </ul>"},{"location":"sandboxing/#security-properties","title":"Security Properties","text":""},{"location":"sandboxing/#what-sandboxing-prevents","title":"What Sandboxing Prevents","text":"<ul> <li>Reading system files (<code>/etc/passwd</code>, <code>/etc/shadow</code>)</li> <li>Reading home directory (<code>~/.ssh/</code>, <code>~/.aws/credentials</code>)</li> <li>Writing outside workspace</li> <li>Accessing secrets in parent directories</li> <li>Path traversal attacks (<code>../../../etc/passwd</code>)</li> <li>Absolute path exploits (<code>/etc/passwd</code>)</li> </ul>"},{"location":"sandboxing/#how-it-works_1","title":"How It Works","text":"<p>All filesystem and exec operations go through <code>SandboxExecutor</code>, which routes them to <code>Sandbox.exec</code>. Each operation spawns a sandboxed process (bubblewrap or Docker) that cannot access files outside the workspace \u2014 enforced by the OS kernel, not application code.</p> <p>Shell escaping (single-quote escaping, base64 encoding for file content) prevents command injection within sandboxed commands.</p> <p>Note: Plugin tools that call external CLIs (e.g. <code>gh</code>, <code>curl</code>) use <code>Process.run</code> with argument arrays (no shell interpretation) and run outside the sandbox since they need host resources (auth configs, SSL certs).</p>"},{"location":"sandboxing/#what-sandboxing-does-not-prevent","title":"What Sandboxing Does NOT Prevent","text":"<ul> <li>Network attacks (agent has network access)</li> <li>API key theft (main process has keys)</li> <li>DoS via API calls</li> <li>Social engineering (user approves actions)</li> </ul> <p>Defense in depth: Use API key scoping, rate limiting, and audit logs.</p>"},{"location":"sandboxing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sandboxing/#error-no-sandbox-tool-found","title":"Error: \"No sandbox tool found\"","text":"<p>Problem: No sandboxing tool installed</p> <p>Fix: <pre><code># Linux: Install bubblewrap\nsudo apt install bubblewrap\n\n# macOS/Windows: Install Docker\n# https://docs.docker.com/engine/install/\n</code></pre></p>"},{"location":"sandboxing/#error-failed-to-start-sandbox","title":"Error: \"Failed to start sandbox\"","text":"<p>Problem: Binary or configuration issues</p> <p>Fix: <pre><code># 1. Verify tools are installed\nwhich bwrap    # Linux\nwhich docker   # macOS/Windows\n\n# 2. Check workspace exists\nls -ld /path/to/workspace\n\n# 3. Try Docker fallback\nautobot agent --sandbox docker\n</code></pre></p>"},{"location":"sandboxing/#development","title":"Development","text":""},{"location":"sandboxing/#running-without-sandbox-tests","title":"Running Without Sandbox (Tests)","text":"<p>Tests automatically disable sandboxing:</p> <pre><code># Tests pass nil workspace to SandboxExecutor\nexecutor = SandboxExecutor.new(nil)\ntool = ReadFileTool.new(executor)\n\n# Tool uses direct file operations (fast, no overhead)\ntool.execute({\"path\" =&gt; JSON::Any.new(\"test.txt\")})  # Direct File.read\n</code></pre>"},{"location":"sandboxing/#testing-sandbox-behavior","title":"Testing Sandbox Behavior","text":"<pre><code># spec/security_spec.cr tests sandbox restrictions\nit \"prevents reading system files\" do\n  executor = SandboxExecutor.new(workspace)\n  tool = ReadFileTool.new(executor)\n  result = tool.execute({\"path\" =&gt; JSON::Any.new(\"/etc/passwd\")})\n  result.error?.should be_true\nend\n</code></pre>"},{"location":"sandboxing/#faq","title":"FAQ","text":"<p>Q: Does this work on Windows? A: Yes, via Docker with WSL2 backend.</p> <p>Q: How do I verify sandboxing works? A: Try reading <code>/etc/passwd</code> - should fail with \"Absolute paths not allowed\"</p> <p>Q: Can I disable sandboxing? A: Only for tests. Production requires sandboxing for safety.</p> <p>Summary: Autobot uses kernel-level sandboxing (bubblewrap or Docker) to restrict LLM file access. Each operation spawns a sandboxed process with shell commands, ensuring compatibility across all platforms with zero extra setup.</p>"},{"location":"security/","title":"Security","text":"<p>Production-grade security checklist for deploying Autobot safely.</p>"},{"location":"security/#1-restrict-who-can-talk-to-the-bot-deny-by-default","title":"1. Restrict Who Can Talk to the Bot (DENY-BY-DEFAULT)","text":"<p>IMPORTANT: Empty <code>allow_from</code> = DENY ALL (secure default since v0.1.0)</p> <pre><code>channels:\n  telegram:\n    allow_from: [\"@alice\", \"123456789\"]  # Allowlist specific users (recommended)\n    # allow_from: [\"*\"]                  # Allow anyone (use with caution!)\n    # allow_from: []                     # Deny all (secure default)\n</code></pre> <p>For Slack: Prefer <code>mention</code> policy in group channels to prevent unauthorized access.</p>"},{"location":"security/#2-kernel-enforced-workspace-sandbox-required-for-production","title":"2. Kernel-Enforced Workspace Sandbox (REQUIRED FOR PRODUCTION)","text":"<p>Autobot uses kernel-level sandboxing to restrict LLM file access to a designated workspace directory.</p>"},{"location":"security/#quick-setup","title":"Quick Setup","text":"<p>Linux (bubblewrap - recommended): <pre><code>sudo apt install bubblewrap  # Ubuntu/Debian\nsudo dnf install bubblewrap  # Fedora\nsudo pacman -S bubblewrap    # Arch\n</code></pre></p> <p>macOS/Windows (Docker): <pre><code># Install Docker Desktop\n# https://docs.docker.com/engine/install/\n</code></pre></p>"},{"location":"security/#configuration","title":"Configuration","text":"<pre><code>tools:\n  sandbox: auto  # auto | bubblewrap | docker | none (default)\n</code></pre>"},{"location":"security/#what-it-protects-against","title":"What It Protects Against","text":"<ul> <li>\u2705 Reading system files (<code>/etc/passwd</code>, <code>~/.ssh/</code>)</li> <li>\u2705 Writing outside workspace</li> <li>\u2705 Path traversal (<code>../../../etc/passwd</code>)</li> <li>\u2705 Absolute path exploits (<code>/etc/passwd</code>)</li> <li>\u2705 Symlink attacks</li> </ul> <p>For detailed information, see docs/sandboxing.md</p>"},{"location":"security/#best-practices","title":"Best Practices","text":"<ul> <li>\u2705 Always use sandboxing in production (<code>sandbox: auto</code> or specific type)</li> <li>\u2705 Install bubblewrap for development (lightweight, fast)</li> <li>\u2705 Use Docker for production deployments</li> <li>\u2705 Keep workspace scoped to a dedicated directory, not your home folder</li> <li>\u26a0\ufe0f Never use <code>sandbox: none</code> in production (development-only)</li> <li>\u26a0\ufe0f Never place <code>.env</code> files inside workspace (blocked automatically)</li> </ul>"},{"location":"security/#3-ssrf-protection-always-enabled","title":"3. SSRF Protection (ALWAYS ENABLED)","text":"<p>Built-in protection against Server-Side Request Forgery:</p> <p>Blocked automatically:</p> <ul> <li>Private IP ranges (10.x, 192.168.x, 172.16-31.x)</li> <li>Localhost/loopback (127.x, ::1)</li> <li>Cloud metadata endpoints (169.254.169.254)</li> <li>Link-local addresses (169.254.x, fe80:)</li> <li>Alternate IP notation (octal: 0177.0.0.1, hex: 0x7f.0.0.1)</li> <li>IPv6 private ranges (fc00::/7, fd00::/8)</li> </ul> <p>All DNS records validated to prevent DNS rebinding attacks.</p>"},{"location":"security/#4-keep-secrets-out-of-files","title":"4. Keep Secrets Out of Files","text":""},{"location":"security/#env-file-protection-v020","title":".env File Protection (v0.2.0+)","text":"<p>Autobot enforces strict <code>.env</code> file protection:</p> <p>Automatic blocks (LLM cannot access):</p> <ul> <li><code>.env</code> files blocked in ReadFileTool (read, list directory)</li> <li><code>.env</code> files blocked in ExecTool (commands like <code>cat .env</code>)</li> <li>Pattern matching: <code>.env</code>, <code>.env.local</code>, <code>.env.production</code>, <code>secrets.env</code>, etc.</li> </ul> <p>Configuration validation (<code>autobot doctor</code>):</p> <ul> <li>\u274c Error: Plaintext secrets in <code>config.yml</code></li> <li>\u274c Error: <code>.env</code> permissions not 0600</li> <li>\u274c Error: <code>.env</code> inside workspace (exposes to LLM)</li> <li>\u26a0\ufe0f Warning: Missing <code>.env</code> file</li> </ul> <p>Example secure config: <pre><code># config.yml (safe for LLM to read)\nproviders:\n  anthropic:\n    api_key: \"${ANTHROPIC_API_KEY}\"  # References .env\n\n# .env (NEVER accessible to LLM)\nANTHROPIC_API_KEY=sk-ant-your-secret-key\n</code></pre></p> <p>File locations:</p> <ul> <li>\u2705 <code>./autobot/.env</code> (outside workspace)</li> <li>\u2705 <code>./.env</code> (outside workspace)</li> <li>\u274c <code>./workspace/.env</code> (inside workspace - BLOCKED by validation)</li> </ul>"},{"location":"security/#log-sanitization","title":"Log Sanitization","text":"<p>Automatic log sanitization redacts:</p> <ul> <li>API keys (sk-ant-, sk-, AKIA, etc.)</li> <li>Bearer tokens</li> <li>OAuth tokens</li> <li>Passwords in URLs/params</li> <li>Authorization headers</li> </ul>"},{"location":"security/#5-configuration-validation-autobot-doctor","title":"5. Configuration Validation (<code>autobot doctor</code>)","text":"<p>Use <code>autobot doctor</code> to verify security configuration before deployment:</p> <pre><code>autobot doctor          # Check for errors and warnings\nautobot doctor --strict # Fail on any warning (CI/CD)\n</code></pre> <p>Security checks performed:</p> <p>\u274c Errors (blocks deployment):</p> <ul> <li>Sandbox enabled but not available</li> <li>Plaintext secrets detected in <code>config.yml</code></li> <li><code>.env</code> file permissions not 0600</li> <li><code>.env</code> file inside workspace directory</li> <li>No LLM provider configured</li> </ul> <p>\u26a0\ufe0f Warnings (review recommended):</p> <ul> <li>Gateway bound to 0.0.0.0 (network exposure)</li> <li>Channel authorization not configured (empty <code>allow_from</code>)</li> <li>Missing <code>.env</code> file</li> <li>Workspace restrictions disabled</li> <li>Channels enabled without tokens</li> </ul> <p>Example output: <pre><code>\u274c ERRORS (1):\n  \u2022 CRITICAL: .env file has insecure permissions (644). Run: chmod 600 /path/.env\n\n\u26a0\ufe0f  WARNINGS (1):\n  \u2022 Gateway is bound to 0.0.0.0 (all network interfaces). Use '127.0.0.1' for localhost-only access.\n\nSummary: 1 errors, 1 warnings, 0 info\n</code></pre></p> <p>Integration:</p> <ul> <li>Run automatically on <code>autobot gateway</code> startup</li> <li>Exit code 1 on errors (stops deployment)</li> <li>Use <code>--strict</code> in CI/CD pipelines to catch warnings</li> </ul>"},{"location":"security/#6-review-logs-monitor-access","title":"6. Review Logs &amp; Monitor Access","text":"<pre><code># Check for ACCESS DENIED (security blocks)\ngrep \"ACCESS DENIED\" ./logs/autobot.log\n\n# Tool activity\ngrep \"Executing tool:\" ./logs/autobot.log\n\n# Token usage\ngrep \"Tokens:\" ./logs/autobot.log\n</code></pre> <p>Log levels:</p> <ul> <li><code>INFO</code> - Successful operations</li> <li><code>WARN</code> - Failed operations or ACCESS DENIED</li> <li><code>ERROR</code> - Exceptions and critical failures</li> </ul>"},{"location":"security/#7-file-permissions-automatic","title":"7. File Permissions (AUTOMATIC)","text":"<p>Autobot automatically sets restrictive permissions on sensitive files:</p> <ul> <li>Config files: <code>0600</code> (user read/write only)</li> <li>Session files: <code>0600</code> (user read/write only)</li> <li>Cron store: <code>0600</code> (user read/write only)</li> <li>Directories: <code>0700</code> (user access only)</li> </ul> <p>Validation:</p> <ul> <li><code>autobot doctor</code> checks <code>.env</code> permissions (must be 0600)</li> <li>Automatic enforcement on file creation</li> </ul>"},{"location":"security/#8-cron-job-isolation-automatic","title":"8. Cron Job Isolation (AUTOMATIC)","text":"<p>Jobs are automatically isolated by owner (channel:chat_id):</p> <ul> <li>Users can only list/remove their own jobs</li> <li>Cross-user tampering prevented</li> </ul>"},{"location":"security/#9-rate-limiting-per-session","title":"9. Rate Limiting (PER-SESSION)","text":"<p>Rate limits are enforced per-session to prevent:</p> <ul> <li>One user exhausting limits for others</li> <li>Abuse of expensive operations (web search, LLM calls)</li> </ul>"},{"location":"security/#10-isolate-runtime","title":"10. Isolate Runtime","text":"<p>Recommended deployment:</p> <ul> <li>Run with least-privileged user account</li> <li>Use containerization (Docker) or systemd service boundaries</li> <li>Bind gateway to localhost only (<code>host: 127.0.0.1</code>) unless external access needed</li> <li>Use reverse proxy with TLS for external access</li> </ul> <p>Production security checklist:</p> <ul> <li>[ ] <code>autobot doctor --strict</code> passes</li> <li>[ ] Dedicated user account (not root)</li> <li>[ ] <code>.env</code> permissions (0600)</li> <li>[ ] <code>.env</code> outside workspace</li> <li>[ ] TLS configured (Let's Encrypt)</li> <li>[ ] Gateway bound to localhost</li> <li>[ ] Reverse proxy for external access</li> <li>[ ] Resource limits configured</li> <li>[ ] Log monitoring enabled</li> </ul>"},{"location":"security/#11-known-limitations","title":"11. Known Limitations","text":"<p>WhatsApp Bridge: WebSocket connection has no authentication (ws://).</p> <ul> <li>Mitigation: Only run bridge on localhost</li> <li>TODO: HMAC/JWT authentication in future release</li> </ul>"},{"location":"slack/","title":"Slack","text":"<p>Autobot connects to Slack via Socket Mode (WebSocket). No public IP or webhook URL needed.</p>"},{"location":"slack/#setup","title":"Setup","text":""},{"location":"slack/#1-create-a-slack-app","title":"1. Create a Slack app","text":"<p>Go to api.slack.com/apps &gt; Create New App &gt; From scratch. Choose a name and workspace.</p>"},{"location":"slack/#2-enable-socket-mode","title":"2. Enable Socket Mode","text":"<p>In your app settings: Socket Mode &gt; toggle Enable Socket Mode &gt; name the token (e.g. \"autobot\") &gt; copy the App Token (<code>xapp-...</code>).</p>"},{"location":"slack/#3-add-bot-scopes","title":"3. Add bot scopes","text":"<p>Go to OAuth &amp; Permissions &gt; Scopes &gt; Bot Token Scopes and add:</p> Scope Purpose <code>app_mentions:read</code> Respond to @mentions <code>chat:write</code> Send messages <code>reactions:write</code> Add emoji reactions <code>im:history</code> Read DMs (optional) <code>channels:history</code> Read channel messages (optional, for <code>open</code> policy)"},{"location":"slack/#4-subscribe-to-events","title":"4. Subscribe to events","text":"<p>Go to Event Subscriptions &gt; toggle Enable Events &gt; Subscribe to bot events and add:</p> Event Purpose <code>app_mention</code> Respond to @mentions in channels <code>message.im</code> Respond to direct messages (optional) <code>message.channels</code> Respond to all channel messages (optional, for <code>open</code> policy)"},{"location":"slack/#5-enable-dms-optional","title":"5. Enable DMs (optional)","text":"<p>Go to App Home &gt; Show Tabs &gt; enable Messages Tab &gt; check \"Allow users to send Slash commands and messages from the messages tab\".</p>"},{"location":"slack/#6-install-the-app","title":"6. Install the app","text":"<p>Go to Install App &gt; Install to Workspace &gt; authorize &gt; copy the Bot Token (<code>xoxb-...</code>).</p> <p>After adding new scopes or events, you must reinstall the app for changes to take effect.</p>"},{"location":"slack/#7-get-your-slack-user-id","title":"7. Get your Slack user ID","text":"<p>Click your profile picture in Slack &gt; Profile &gt; click the three dots (...) &gt; Copy member ID (e.g. <code>U02FFF68WGL</code>).</p>"},{"location":"slack/#8-configure","title":"8. Configure","text":"<p>Add tokens to your <code>.env</code> file:</p> <pre><code>SLACK_BOT_TOKEN=xoxb-...\nSLACK_APP_TOKEN=xapp-...\n</code></pre> <p>In <code>config.yml</code>:</p> <pre><code>channels:\n  slack:\n    enabled: true\n    bot_token: \"${SLACK_BOT_TOKEN}\"\n    app_token: \"${SLACK_APP_TOKEN}\"\n    allow_from: [\"U02FFF68WGL\"]  # your Slack user ID\n    group_policy: \"mention\"\n</code></pre>"},{"location":"slack/#9-start","title":"9. Start","text":"<pre><code>autobot agent\n# Should show: Slack bot connected as U...\n#              Connecting to Slack WebSocket...\n</code></pre> <p>Invite the bot to a channel (<code>/invite @your-bot</code>) and mention it to start chatting.</p>"},{"location":"slack/#access-control","title":"Access control","text":"<p>Two layers of access control:</p>"},{"location":"slack/#user-allowlist-allow_from","title":"User allowlist (<code>allow_from</code>)","text":"<p>Controls which Slack users can interact with the bot at all \u2014 across channels and DMs:</p> <pre><code># Deny all (secure default)\nallow_from: []\n\n# Allow specific users (recommended)\nallow_from: [\"U02FFF68WGL\", \"U0AG5U7JLB0\"]\n\n# Allow anyone (use with caution)\nallow_from: [\"*\"]\n</code></pre>"},{"location":"slack/#channel-policy-group_policy","title":"Channel policy (<code>group_policy</code>)","text":"<p>Controls how the bot responds in channels:</p> Policy Behavior <code>\"mention\"</code> Only responds to @mentions (default, secure) <code>\"open\"</code> Responds to all messages in channels <code>\"allowlist\"</code> Only responds in channels listed in <code>group_allow_from</code> <pre><code># Only respond in specific channels\ngroup_policy: \"allowlist\"\ngroup_allow_from: [\"C01ABC123\", \"C02DEF456\"]\n</code></pre>"},{"location":"slack/#dm-policy","title":"DM policy","text":"<p>Controls direct message behavior:</p> <pre><code>dm:\n  enabled: true\n  policy: \"open\"          # \"open\" or \"allowlist\"\n  allow_from: [\"U12345\"]  # required if policy is \"allowlist\"\n</code></pre> <p>DMs are disabled by default. When enabled with <code>allowlist</code> policy, only users in <code>dm.allow_from</code> can DM the bot.</p>"},{"location":"slack/#features","title":"Features","text":"<ul> <li>Socket Mode \u2014 WebSocket connection, no public IP needed</li> <li>@mention responses \u2014 responds when mentioned in channels</li> <li>Thread support \u2014 replies in threads for channel messages</li> <li>DM support \u2014 configurable direct message handling</li> <li>Emoji reactions \u2014 adds :eyes: to received messages</li> <li>Auto-reconnect \u2014 reconnects automatically on disconnection</li> </ul>"},{"location":"slack/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>enabled</code> No <code>false</code> Enable the Slack channel <code>bot_token</code> Yes \u2014 Bot User OAuth Token (<code>xoxb-...</code>) <code>app_token</code> Yes \u2014 App-Level Token (<code>xapp-...</code>) <code>allow_from</code> No <code>[]</code> Slack user IDs allowed to use the bot <code>mode</code> No <code>\"socket\"</code> Connection mode <code>group_policy</code> No <code>\"mention\"</code> Channel response policy <code>group_allow_from</code> No <code>[]</code> Channel IDs for allowlist policy <code>dm.enabled</code> No <code>false</code> Enable direct messages <code>dm.policy</code> No <code>\"allowlist\"</code> DM policy: <code>\"allowlist\"</code> or <code>\"open\"</code> <code>dm.allow_from</code> No <code>[]</code> User IDs for DM allowlist policy"},{"location":"slack/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent\n</code></pre> <p>\"Access denied for sender U...\" \u2014 the user ID is not in <code>allow_from</code>. Add it to the config.</p> <p>Bot connects but doesn't respond to mentions \u2014 reinstall the app after adding event subscriptions. Go to Install App &gt; Reinstall to Workspace.</p> <p>\"Sending messages to this app has been turned off\" \u2014 enable the Messages Tab in App Home &gt; Show Tabs.</p> <p>\"Slack bot/app token not configured\" \u2014 tokens are empty. Check <code>.env</code> file and variable substitution.</p> <p>\"Failed to open Slack connection\" \u2014 the app token is invalid or Socket Mode is not enabled. Check Socket Mode settings.</p>"},{"location":"telegram/","title":"Telegram","text":"<p>Autobot connects to Telegram via the Bot API using long polling. No webhook or public IP needed.</p>"},{"location":"telegram/#setup","title":"Setup","text":""},{"location":"telegram/#1-create-a-bot","title":"1. Create a bot","text":"<p>Open Telegram and message @BotFather:</p> <ol> <li>Send <code>/newbot</code></li> <li>Choose a display name (e.g. \"My Autobot\")</li> <li>Choose a username (must end in <code>bot</code>, e.g. <code>my_autobot_bot</code>)</li> <li>Copy the bot token (<code>123456:ABC-DEF...</code>)</li> </ol>"},{"location":"telegram/#2-get-your-user-id","title":"2. Get your user ID","text":"<p>Message @userinfobot \u2014 it replies with your numeric user ID (e.g. <code>123456789</code>).</p>"},{"location":"telegram/#3-configure","title":"3. Configure","text":"<p>Add the token to your <code>.env</code> file:</p> <pre><code>TELEGRAM_BOT_TOKEN=123456:ABC-DEF...\n</code></pre> <p>In <code>config.yml</code>:</p> <pre><code>channels:\n  telegram:\n    enabled: true\n    token: \"${TELEGRAM_BOT_TOKEN}\"\n    allow_from: [\"123456789\"]  # your user ID\n</code></pre>"},{"location":"telegram/#4-start","title":"4. Start","text":"<pre><code>autobot agent\n# Should show: Telegram bot @my_autobot_bot connected\n</code></pre> <p>Open a chat with your bot in Telegram and send a message.</p>"},{"location":"telegram/#access-control","title":"Access control","text":"<p><code>allow_from</code> controls who can interact with the bot. It accepts Telegram user IDs and usernames:</p> <pre><code># Deny all (secure default)\nallow_from: []\n\n# Allow specific users (recommended)\nallow_from: [\"123456789\", \"username\"]\n\n# Allow anyone (use with caution)\nallow_from: [\"*\"]\n</code></pre> <p>Telegram sends both numeric user ID and username. The bot matches against both \u2014 <code>\"123456789\"</code> and <code>\"johndoe\"</code> both work.</p> <p>Unauthorized users receive a friendly denial message with their user ID, so they can share it with you to be added.</p>"},{"location":"telegram/#custom-commands","title":"Custom commands","text":"<p>Add custom slash commands that appear in Telegram's command menu:</p> <pre><code>channels:\n  telegram:\n    enabled: true\n    token: \"${TELEGRAM_BOT_TOKEN}\"\n    allow_from: [\"123456789\"]\n    custom_commands:\n      macros:\n        summarize: \"Summarize the last conversation in 3 bullet points\"\n        translate:\n          prompt: \"Translate the following to English\"\n          description: \"Translate text to English\"\n      scripts:\n        deploy:\n          path: \"/home/user/scripts/deploy.sh\"\n          description: \"Deploy to production\"\n</code></pre> <p>Macros send the prompt to the LLM. Scripts execute a shell command and return the output.</p>"},{"location":"telegram/#built-in-commands","title":"Built-in commands","text":"Command Description <code>/start</code> Welcome message <code>/reset</code> Clear conversation history <code>/help</code> List available commands"},{"location":"telegram/#features","title":"Features","text":"<ul> <li>Long polling \u2014 no webhook or public IP needed</li> <li>Voice messages \u2014 auto-transcribed via Whisper (requires Groq or OpenAI provider)</li> <li>Photos \u2014 sent as image attachments to the LLM</li> <li>Documents \u2014 attached to the message context</li> <li>Typing indicators \u2014 shows \"typing...\" while the LLM responds</li> <li>Markdown rendering \u2014 LLM responses are converted to Telegram HTML</li> <li>Group chats \u2014 bot responds when mentioned in groups</li> </ul>"},{"location":"telegram/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>enabled</code> No <code>false</code> Enable the Telegram channel <code>token</code> Yes \u2014 Bot API token from BotFather <code>allow_from</code> No <code>[]</code> User IDs/usernames allowed to use the bot <code>proxy</code> No \u2014 HTTP proxy URL for API requests <code>custom_commands</code> No \u2014 Custom slash commands (macros and scripts)"},{"location":"telegram/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent\n</code></pre> <p>Bot doesn't respond \u2014 check <code>allow_from</code> contains your user ID. The log shows <code>Access denied for sender &lt;id&gt;</code> with the exact ID to add.</p> <p>\"Telegram bot token not configured\" \u2014 token is empty. Check <code>.env</code> file and <code>${TELEGRAM_BOT_TOKEN}</code> substitution.</p> <p>Voice messages show <code>[voice message]</code> \u2014 no Whisper provider configured. Add Groq or OpenAI provider.</p>"},{"location":"vllm/","title":"vLLM / Local models","text":"<p>Autobot supports vLLM and any OpenAI-compatible local inference server as an LLM provider. This lets you run models on your own hardware with full privacy and no API costs.</p>"},{"location":"vllm/#setup","title":"Setup","text":""},{"location":"vllm/#1-start-a-local-server","title":"1. Start a local server","text":"<p>Start a vLLM server (or any OpenAI-compatible endpoint):</p> <pre><code># vLLM\nvllm serve meta-llama/Llama-3.3-70B-Instruct --port 8000\n\n# Ollama (OpenAI-compatible mode)\nollama serve  # Runs on port 11434\n\n# llama.cpp server\nllama-server -m model.gguf --port 8080\n</code></pre>"},{"location":"vllm/#2-configure-the-provider","title":"2. Configure the provider","text":"<p>In <code>config.yml</code>:</p> <pre><code>agents:\n  defaults:\n    model: \"vllm/meta-llama/Llama-3.3-70B-Instruct\"\n\nproviders:\n  vllm:\n    api_base: \"http://localhost:8000\"\n    api_key: \"token\"\n</code></pre> <p>The <code>api_key</code> field is required by the config schema but most local servers ignore it. Use any non-empty value (e.g. <code>\"token\"</code> or <code>\"none\"</code>).</p>"},{"location":"vllm/#3-verify","title":"3. Verify","text":"<pre><code>autobot doctor\n# Should show: \u2713 LLM provider configured (vllm)\n</code></pre>"},{"location":"vllm/#model-naming","title":"Model naming","text":"<p>For local providers, the model name after the <code>vllm/</code> prefix should match what the server expects:</p> <pre><code># vLLM \u2014 uses the model name from the serve command\nmodel: \"vllm/meta-llama/Llama-3.3-70B-Instruct\"\n\n# Ollama \u2014 uses the model tag\nmodel: \"vllm/llama3.3:70b\"\n\n# llama.cpp \u2014 usually any string works (model is already loaded)\nmodel: \"vllm/local\"\n</code></pre>"},{"location":"vllm/#endpoint-configuration","title":"Endpoint configuration","text":"<p>The <code>api_base</code> should point to your server's base URL. Autobot appends <code>/chat/completions</code> automatically:</p> <pre><code># vLLM (default port 8000)\napi_base: \"http://localhost:8000\"\n# -&gt; POST http://localhost:8000/chat/completions\n\n# Ollama (default port 11434)\napi_base: \"http://localhost:11434/v1\"\n# -&gt; POST http://localhost:11434/v1/chat/completions\n\n# Custom server with full path\napi_base: \"http://localhost:8080/v1/chat/completions\"\n# -&gt; POST http://localhost:8080/v1/chat/completions (used as-is)\n</code></pre> <p>If the <code>api_base</code> already ends with <code>/chat/completions</code>, it is used as-is.</p>"},{"location":"vllm/#configuration-reference","title":"Configuration reference","text":"Field Required Default Description <code>api_key</code> Yes* \u2014 API key or token (most local servers ignore it, use any non-empty value) <code>api_base</code> Yes \u2014 Server URL (e.g. <code>http://localhost:8000</code>) <code>extra_headers</code> No \u2014 Additional HTTP headers for every request <p>*Required by config schema, but the value typically does not matter for local servers.</p>"},{"location":"vllm/#how-it-works","title":"How it works","text":"<p>vLLM and other local servers implement the OpenAI-compatible Chat Completions API:</p> <ul> <li><code>Authorization: Bearer</code> header sent (most local servers ignore it)</li> <li>Standard message format with <code>role</code> and <code>content</code> fields</li> <li>Function calling works if the served model supports it</li> </ul> <p>Autobot detects local providers by the <code>vllm</code> keyword or by explicit <code>provider_name</code> in config. The OpenAI-compatible request format is used for all local servers.</p>"},{"location":"vllm/#voice-transcription","title":"Voice transcription","text":"<p>Local servers do not provide a transcription API. If you need voice message support, configure an additional Groq or OpenAI provider for Whisper-based transcription.</p>"},{"location":"vllm/#known-limitations","title":"Known limitations","text":"<ul> <li>No streaming \u2014 Responses are returned in full after the model finishes generating.</li> <li>Tool support varies \u2014 Function calling depends on the model and server. Not all local models support tools.</li> <li>Tool choice is always <code>auto</code> \u2014 There is no configuration to force a specific tool or disable tool use per-request.</li> <li>No automatic model detection \u2014 You must specify the exact model name the server expects.</li> </ul>"},{"location":"vllm/#troubleshooting","title":"Troubleshooting","text":"<p>Enable debug logging to see request/response details:</p> <pre><code>LOG_LEVEL=DEBUG autobot agent -m \"Hello\"\n</code></pre> <p>Look for:</p> <ul> <li><code>POST http://localhost:8000/chat/completions model=...</code> \u2014 confirms provider is active</li> <li><code>Response 200 (N bytes)</code> \u2014 confirms server response</li> <li><code>LLM request failed: ...</code> \u2014 connection or request errors</li> </ul>"},{"location":"vllm/#common-issues","title":"Common issues","text":"<p>\"No LLM provider configured\" \u2014 Check that <code>api_key</code> is set (any non-empty value) and <code>api_base</code> points to your running server.</p> <p>\"LLM request failed: Connection refused\" \u2014 Server is not running or the port is wrong. Verify the server is up with <code>curl http://localhost:8000/v1/models</code>.</p> <p>\"API error: model not found\" \u2014 The model name in config doesn't match what the server is serving. Check available models with <code>curl http://localhost:8000/v1/models</code>.</p> <p>Slow responses \u2014 Local inference speed depends on your hardware (GPU/CPU). Smaller or quantized models run faster.</p>"},{"location":"web-search/","title":"Web Search &amp; Fetch","text":"<p>Autobot includes two built-in web tools that give the LLM access to the internet: <code>web_search</code> for querying search engines and <code>web_fetch</code> for retrieving page content.</p>"},{"location":"web-search/#how-it-works","title":"How It Works","text":"<pre><code>User message -&gt; Agent loop -&gt; LLM requests web_search/web_fetch -&gt; Tool executes -&gt; Results fed back to LLM\n</code></pre> <ol> <li>The LLM decides it needs external information and calls <code>web_search</code> or <code>web_fetch</code></li> <li>The tool executes the request (search query or URL fetch)</li> <li>Results are returned to the LLM as tool output</li> <li>The LLM incorporates the information into its response</li> </ol> <p>Both tools are registered automatically in the agent's tool registry at startup.</p>"},{"location":"web-search/#tools","title":"Tools","text":""},{"location":"web-search/#web_search","title":"web_search","text":"<p>Searches the web using the Brave Search API.</p> <p>Parameters:</p> Parameter Type Required Description <code>query</code> string yes Search query <code>count</code> integer no Number of results (1-10, default: 5) <p>Returns: Numbered list of results with title, URL, and description snippet.</p>"},{"location":"web-search/#web_fetch","title":"web_fetch","text":"<p>Fetches a URL and extracts readable text content. Supports HTML (with tag stripping), JSON (pretty-printed), and raw text.</p> <p>Parameters:</p> Parameter Type Required Description <code>url</code> string yes URL to fetch (http/https only) <code>maxChars</code> integer no Max content chars to return (default: 20,000) <p>Returns: Plain text with URL header and extracted content. Includes truncation notice when content exceeds <code>maxChars</code>.</p> <p>Features:</p> <ul> <li>Follows redirects (max 5 hops)</li> <li>HTML tag stripping with entity decoding</li> <li>JSON pretty-printing</li> <li>10-second read/connect timeout</li> </ul>"},{"location":"web-search/#configuration","title":"Configuration","text":"<pre><code>tools:\n  web:\n    search:\n      api_key: \"${BRAVE_API_KEY}\"\n      max_results: 5\n</code></pre> <p>The Brave API key can also be set via the <code>BRAVE_API_KEY</code> environment variable. If no key is configured, <code>web_search</code> returns an error message \u2014 <code>web_fetch</code> works without any API key.</p>"},{"location":"web-search/#security","title":"Security","text":""},{"location":"web-search/#ssrf-protection","title":"SSRF Protection","text":"<p><code>web_fetch</code> includes defense against Server-Side Request Forgery (SSRF) attacks. Before connecting to any URL, the tool:</p> <ol> <li>Validates the scheme \u2014 only <code>http</code> and <code>https</code> are allowed</li> <li>Resolves DNS and validates all returned IPs (not just the first)</li> <li>Blocks private ranges \u2014 RFC 1918 (<code>10.x</code>, <code>172.16-31.x</code>, <code>192.168.x</code>), IPv6 ULA (<code>fc00::/7</code>)</li> <li>Blocks loopback \u2014 <code>127.x</code>, <code>::1</code>, <code>0.0.0.0</code></li> <li>Blocks cloud metadata \u2014 <code>169.254.169.254</code> (AWS/GCP/Azure metadata endpoint)</li> <li>Blocks link-local \u2014 <code>169.254.x</code>, <code>fe80:</code></li> <li>Blocks alternate IP notation \u2014 octal (<code>0177.0.0.1</code>), hex (<code>0x7f000001</code>), integer notation</li> <li>Validates redirect targets \u2014 each redirect hop is re-validated against all SSRF checks</li> <li>Connects to validated IP \u2014 prevents DNS rebinding by connecting to the resolved IP directly</li> </ol>"},{"location":"web-search/#rate-limiting","title":"Rate Limiting","text":"<p>Both tools are subject to the global tool rate limiter, preventing excessive API calls within a session.</p>"},{"location":"zulip/","title":"Zulip Channel","text":"<p>Autobot can connect to Zulip using its Real-time Events API (Long Polling).</p> <p>This channel uses a bot account, so you'll need to configure a bot of the type \"Generic bot\" in Zulip before Autobot can connect to it.</p>"},{"location":"zulip/#features","title":"Features","text":"<ul> <li>Direct Messages \u2014 Full conversation support via private messages.</li> <li>Security \u2014 Email-based allowlisting to control who can interact with the bot.</li> </ul>"},{"location":"zulip/#configuration","title":"Configuration","text":"<p>Add a <code>zulip</code> block to the <code>channels</code> section of your <code>config.yml</code>:</p> <pre><code>channels:\n  zulip:\n    enabled: true\n    site: \"https://zulip.example.com\"\n    email: \"bot-email@zulip.example.com\"\n    api_key: \"your-api-key\"\n    # allow_from: []          - DENY ALL (secure default)\n    # allow_from: [\"*\"]       - Allow anyone (use with caution)\n    # allow_from: [\"user@example.com\"] - Allowlist specific emails\n    allow_from: [\"you@example.com\"]\n</code></pre>"},{"location":"zulip/#parameters","title":"Parameters","text":"Parameter Required Default Description <code>enabled</code> No <code>false</code> Enable the Zulip channel <code>site</code> Yes - Your Zulip server URL (e.g., <code>https://zulip.example.com</code>) <code>email</code> Yes - The email address of the Zulip bot <code>api_key</code> Yes - The API key for the Zulip bot <code>allow_from</code> No <code>[]</code> List of authorized user emails or <code>[\"*\"]</code> for all"},{"location":"zulip/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Create a Bot on Zulip:</li> <li>Go to Personal settings -&gt; Bots -&gt; Add a new bot.</li> <li>Choose a bot type (e.g., \"Generic bot\").</li> <li> <p>Note the bot's email address and API key.</p> </li> <li> <p>Configure Autobot:</p> </li> <li>Add the <code>site</code>, <code>email</code>, and <code>api_key</code> to your <code>config.yml</code>.</li> <li> <p>Add your own email address to <code>allow_from</code>.</p> </li> <li> <p>Start Autobot:</p> </li> <li>Run <code>autobot gateway</code>.</li> <li>Send a direct message to your bot on Zulip.</li> </ol>"},{"location":"zulip/#limitations","title":"Limitations","text":"<ul> <li>Currently only supports private direct messages. Stream messages are not yet supported.</li> <li>Media handling (photos, attachments) is not yet implemented for Zulip.</li> </ul>"}]}